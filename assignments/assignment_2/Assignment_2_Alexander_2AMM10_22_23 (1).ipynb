{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d32f8d18",
      "metadata": {
        "id": "d32f8d18"
      },
      "source": [
        "# Group Details\n",
        "\n",
        "## Group Name:\n",
        "\n",
        "### Student 1:\n",
        "\n",
        "### Student 2:\n",
        "\n",
        "### Student 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faec2056",
      "metadata": {
        "id": "faec2056"
      },
      "source": [
        "# Loading Data and Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "notebook_dir = \"/content/drive/MyDrive/Colab Notebooks/data/\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tMKvvfDBpAK",
        "outputId": "58caf6aa-3774-4307-8a68-d897ce33c9c4"
      },
      "id": "5tMKvvfDBpAK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d0580a5",
      "metadata": {
        "id": "7d0580a5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import torch_geometric\n",
        "except:\n",
        "    !pip install torch_geometric\n",
        "    import torch_geometric"
      ],
      "metadata": {
        "id": "PnbALjc2sVe-"
      },
      "id": "PnbALjc2sVe-",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b0756591",
      "metadata": {
        "id": "b0756591"
      },
      "outputs": [],
      "source": [
        "def load_array(filename, task):\n",
        "    datapoint = np.load(filename)\n",
        "    if task == 'task 1':\n",
        "        initial_state = datapoint['initial_state']\n",
        "        terminal_state = datapoint['terminal_state']\n",
        "        return initial_state, terminal_state\n",
        "    elif task == 'task 2' or task == 'task 3':\n",
        "        whole_trajectory = datapoint['trajectory']\n",
        "        # change shape: (num_bodies, attributes, time) ->  num_bodies, time, attributes\n",
        "        whole_trajectory = np.swapaxes(whole_trajectory, 1, 2)\n",
        "        initial_state = whole_trajectory[:, 0]\n",
        "        target = whole_trajectory[:, 1:, 1:]  # drop the first timepoint (second dim) and mass (last dim) for the prediction task\n",
        "        return initial_state, target\n",
        "    else:\n",
        "        raise NotImplementedError(\"'task' argument should be 'task 1', 'task 2' or 'task 3'!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bb77a4be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb77a4be",
        "outputId": "fe62ba6e-6030-4d13-8554-cf2fd28a0916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of initial state (model input): (8, 5)\n",
            "shape of terminal state (to be predicted by model): (8, 2)\n",
            "The initial x-coordinate of the body with index 2 in this trajectory was -5.159721083543527\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This cell gives an example of loading a datapoint with numpy for task 1.\n",
        "\n",
        "The arrays returned by the function are structures as follows:\n",
        "initial_state: shape (n_bodies, [mass, x, y, v_x, v_y])\n",
        "terminal_state: shape (n_bodies, [x, y])\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "example1 = load_array(notebook_dir+'data/task 1/train/trajectory_0.npz', task='task 1')\n",
        "\n",
        "initial_state, terminal_state = example1\n",
        "print(f'shape of initial state (model input): {initial_state.shape}')\n",
        "print(f'shape of terminal state (to be predicted by model): {terminal_state.shape}')\n",
        "\n",
        "body_idx = 2\n",
        "print(f'The initial x-coordinate of the body with index {body_idx} in this trajectory was {initial_state[body_idx, 1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1c3ea4cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c3ea4cb",
        "outputId": "a2cbf9bd-8bbd-48a6-c0d6-a6df71a04acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of initial state (model input): (8, 5)\n",
            "shape of terminal state (to be predicted by model): (8, 49, 4)\n",
            "The y-coordinate of the body with index 2 at time with index 30 in remaining_trajectory was -0.3861544940435097\n",
            "the shape of the input of a test data example is (8, 5)\n",
            "the shape of the target of a test data example is (8, 49, 4)\n",
            "values of the test data example at time 30:\n",
            " [[-5.85725792 -5.394571           nan         nan]\n",
            " [-6.03781257 -5.72445953         nan         nan]\n",
            " [-0.90623054 -6.93416278         nan         nan]\n",
            " [ 2.83149339 -7.50100819         nan         nan]\n",
            " [-2.85586881  1.77667501         nan         nan]\n",
            " [ 4.04424526  4.00563603         nan         nan]\n",
            " [-5.24887713 -4.83081005         nan         nan]\n",
            " [-5.81391023 -5.1109838          nan         nan]]\n",
            "note: velocity values are unobserved (NaNs) in the test data!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This cell gives an example of loading a datapoint with numpy for task 2 / 3.\n",
        "\n",
        "The arrays returned by the function are structures as follows:\n",
        "initial_state: shape (n_bodies, [mass, x, y, v_x, v_y])\n",
        "remaining_trajectory: shape (n_bodies, time, [x, y, v_x, v_y])\n",
        "\n",
        "Note that for this task, you are asked to evaluate performance only with regard to the predictions of the positions (x and y).\n",
        "If you use the velocity of the remaining trajectory for training,\n",
        "this use should be purely auxiliary for the goal of predicting the positions [x,y] over time.\n",
        "While testing performance of your model on the test set, you do not have access to v_x and v_y of the remaining trajectory.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "example2 = load_array(notebook_dir+'data/task 2_3/train/trajectory_0.npz', task='task 2')\n",
        "\n",
        "initial_state, remaining_trajectory = example2\n",
        "print(f'shape of initial state (model input): {initial_state.shape}')\n",
        "print(f'shape of terminal state (to be predicted by model): {remaining_trajectory.shape}')\n",
        "\n",
        "body_idx = 2\n",
        "time_idx = 30\n",
        "print(f'The y-coordinate of the body with index {body_idx} at time with index {time_idx} in remaining_trajectory was {remaining_trajectory[body_idx, time_idx, 1]}')\n",
        "\n",
        "test_example = load_array(notebook_dir+'data/task 2_3/test/trajectory_900.npz', task='task 3')\n",
        "test_initial_state, test_remaining_trajectory = test_example\n",
        "print(f'the shape of the input of a test data example is {test_initial_state.shape}')\n",
        "print(f'the shape of the target of a test data example is {test_remaining_trajectory.shape}')\n",
        "print(f'values of the test data example at time {time_idx}:\\n {test_remaining_trajectory[:, time_idx]}')\n",
        "print('note: velocity values are unobserved (NaNs) in the test data!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "id": "f9106543",
      "metadata": {
        "id": "f9106543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5249172c-f2c4-4540-9b62-1f684c889718"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[-5.27118739,  5.07863417],\n",
              "        [ 4.3270607 , -0.08095022],\n",
              "        [-5.15972108,  5.35238208],\n",
              "        [-6.79584511,  2.29632123],\n",
              "        [-2.75861066,  1.77940931],\n",
              "        [ 0.34981219,  4.8286224 ],\n",
              "        [-0.50562258, 15.04631712],\n",
              "        [ 1.81342682,  3.84375499]]),\n",
              " array([[-10.75571877, -13.26786402],\n",
              "        [  1.4336244 , -14.87951239],\n",
              "        [  6.97288106,  12.64591849],\n",
              "        [ -5.27493319,   0.26332222],\n",
              "        [ -3.140792  ,  -0.01830862],\n",
              "        [  7.13136029,  -7.9594037 ],\n",
              "        [ -5.24424874,  89.95271045],\n",
              "        [ -1.6814287 ,  -7.41086554]]),\n",
              " array([[-5.38087802,  4.71170421],\n",
              "        [-5.49056864,  4.34477425],\n",
              "        [-5.60025927,  3.97784428],\n",
              "        [-5.7099499 ,  3.61091432],\n",
              "        [-5.81964053,  3.24398435]]))"
            ]
          },
          "metadata": {},
          "execution_count": 216
        }
      ],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "def constant_baseline(X):\n",
        "    \"\"\" x_t = x_0 \"\"\"\n",
        "    predictions = []\n",
        "    for i in range(len(X)):\n",
        "        mass, x, y, v_x, v_y = X[i]\n",
        "        final_coords = [x, y]\n",
        "        predictions.append(final_coords)\n",
        "    return np.array(predictions)\n",
        "\n",
        "def linear_baseline(X, t=5):\n",
        "    \"\"\"x_t = x_0 + v_0 * t\"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    for i in range(len(X)):\n",
        "        mass, x, y, v_x, v_y = X[i]\n",
        "        final_coords = [x+v_x*t, y+v_y*t]\n",
        "        predictions.append(final_coords)\n",
        "    return np.array(predictions)\n",
        "\n",
        "def sequence_baseline(X, t=50):\n",
        "    predictions = []\n",
        "    for i in range(len(X)):\n",
        "        seq = []\n",
        "        for j in range(t):\n",
        "            outp = linear_baseline([X[i]], ((j+1)*0.1))[0]\n",
        "            seq.append(list(outp))\n",
        "        predictions.append(seq)\n",
        "    return np.array(predictions)\n",
        "\n",
        "def obtain_gt(y):\n",
        "    truth = []\n",
        "    for i in range(len(y)):\n",
        "        seq = []\n",
        "        for j in range(len(y[i])):\n",
        "            seq.append([y[i][j][0], y[i][j][1]])\n",
        "        truth.append(seq)\n",
        "    return np.array(truth)\n",
        "\n",
        "system = example1[0]\n",
        "constant_baseline(system), linear_baseline(system, t=5), sequence_baseline(system)[0][:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059b633c",
      "metadata": {
        "id": "059b633c"
      },
      "source": [
        "# Data Handling and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "t1_train = notebook_dir+'data/task 1/train/'\n",
        "t1_test = notebook_dir+'data/task 1/test/'\n",
        "t23_train = notebook_dir+'data/task 2_3/train/'\n",
        "t23_test = notebook_dir+'data/task 2_3/test/'\n",
        "\n",
        "t1_train_files = os.listdir(t1_train)\n",
        "t1_test_files = os.listdir(t1_test)\n",
        "t23_train_files = os.listdir(t23_train)\n",
        "t23_test_files = os.listdir(t23_test)"
      ],
      "metadata": {
        "id": "LtQORsSpDKoi"
      },
      "id": "LtQORsSpDKoi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "\n",
        "class ImportData(Dataset):\n",
        "    def __init__(self, folder_path):\n",
        "        self.folder_path = folder_path\n",
        "        self.file_list = sorted(os.listdir(folder_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_name = self.file_list[index]\n",
        "        file_path = os.path.join(self.folder_path, file_name)\n",
        "        data, label = load_array(file_path, task='task 1')\n",
        "        return data, label\n",
        "\n",
        "# Create an instance of the custom dataset class with the folder path\n",
        "train_import = ImportData(t1_train)\n",
        "test_import = ImportData(t1_test)\n",
        "\n",
        "X_train_import = []\n",
        "y_train_import = []\n",
        "X_test_import = []\n",
        "y_test_import = []\n",
        "\n",
        "# Iterate through the train_dataset to extract data and labels\n",
        "for data, label in train_import:\n",
        "    X_train_import.append(data)\n",
        "    y_train_import.append(label)\n",
        "\n",
        "for data, label in test_import:\n",
        "    X_test_import.append(data)\n",
        "    y_test_import.append(label)\n",
        "\n",
        "max_length = 9\n",
        "\n",
        "# Pad the data samples with zeros to have the same shape\n",
        "X_train_padded = []\n",
        "for data in X_train_import:\n",
        "    pad_width = max_length - data.shape[0]\n",
        "    padded_data = np.pad(data, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    X_train_padded.append(padded_data)\n",
        "\n",
        "y_train_padded = []\n",
        "for label in y_train_import:\n",
        "    pad_width = max_length - label.shape[0]\n",
        "    padded_label = np.pad(label, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    y_train_padded.append(padded_label)\n",
        "\n",
        "X_test_padded = []\n",
        "for data in X_test_import:\n",
        "    pad_width = max_length - data.shape[0]\n",
        "    padded_data = np.pad(data, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    X_test_padded.append(padded_data)\n",
        "\n",
        "y_test_padded = []\n",
        "for label in y_test_import:\n",
        "    pad_width = max_length - label.shape[0]\n",
        "    padded_label = np.pad(label, ((0, pad_width), (0, 0)), mode='constant')\n",
        "    y_test_padded.append(padded_label)\n",
        "\n",
        "# Convert the padded data and labels to tensors\n",
        "X_train = torch.tensor(np.array(X_train_padded))\n",
        "y_train = torch.tensor(np.array(y_train_padded))\n",
        "X_test = torch.tensor(np.array(X_test_padded))\n",
        "y_test = torch.tensor(np.array(y_test_padded))\n",
        "\n",
        "# Print the shape of X_train and the first label in y_train\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L3zm_UKC9JD",
        "outputId": "8658d677-1ca1-4028-9d60-8089cd6319ec"
      },
      "id": "9L3zm_UKC9JD",
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape:  torch.Size([900, 9, 5])\n",
            "y_train shape:  torch.Size([900, 9, 2])\n",
            "X_test shape:  torch.Size([100, 9, 5])\n",
            "y_test shape:  torch.Size([100, 9, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Function to process a single data point\n",
        "def create_graph(data_point, label):\n",
        "\n",
        "    #Create node features\n",
        "    node_features = data_point  # Exclude the mass column\n",
        "\n",
        "    # Compute edge indices\n",
        "    edge_indices = []\n",
        "    for i in range(9):\n",
        "        for j in range(9):\n",
        "            if i != j:\n",
        "                edge_indices.append([i, j])\n",
        "    edge_indices = torch.tensor(edge_indices).t().contiguous()\n",
        "\n",
        "    # Compute edge features\n",
        "    edge_features = []\n",
        "    for i in range(9):\n",
        "        for j in range(9):\n",
        "            if i != j:\n",
        "                pos = node_features[j] - node_features[i]\n",
        "\n",
        "                # Calculate Euclidean distance\n",
        "                # pos = torch.tensor(pos)\n",
        "                relative_position = torch.norm(pos)\n",
        "                # print(\"Relative position: \", relative_position)\n",
        "\n",
        "                relative_mass = node_features[j, 0] / node_features[i, 0]\n",
        "                if relative_mass == np.inf or relative_mass == 0:\n",
        "                    relative_position = 0\n",
        "                    relative_mass = 0\n",
        "                # relative_mass = torch.tensor(relative_mass)\n",
        "                # print(\"Relative mass: \", relative_mass)\n",
        "                edge_features.append([relative_position, relative_mass])\n",
        "                # edge_features.append(torch.cat((relative_position, torch.tensor([relative_mass], dtype=torch.float32))))\n",
        "    edge_features = torch.FloatTensor(edge_features)\n",
        "\n",
        "    data = Data(x=node_features, edge_index=edge_indices, edge_attr=edge_features, y=label)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "FVYhGo1eJker"
      },
      "id": "FVYhGo1eJker",
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = create_graph(X_train[0], y_train[0])\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3eldywTDwyX",
        "outputId": "aa81c421-04f9-4800-eb9b-43596dddc8a4"
      },
      "id": "W3eldywTDwyX",
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[9, 5], edge_index=[2, 72], edge_attr=[72, 2], y=[9, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_graphs_lst = []\n",
        "for i in range(len(X_train)):\n",
        "    train_graphs_lst.append(create_graph(X_train[i], y_train[i]))\n",
        "\n",
        "test_graphs_lst = []\n",
        "for i in range(len(X_test)):\n",
        "    test_graphs_lst.append(create_graph(X_test[i], y_test[i]))"
      ],
      "metadata": {
        "id": "60LDmUx1GwYk"
      },
      "id": "60LDmUx1GwYk",
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_edge(length, i):\n",
        "#     lst1 = []\n",
        "#     lst1.extend([i]*(length-1))\n",
        "#     lst2 = []\n",
        "#     for j in range(length):\n",
        "#         if i != j:\n",
        "#             lst2.append(j)\n",
        "#     return np.array([lst2, lst1])\n",
        "\n",
        "# def create_edges(length):\n",
        "#     lst1 = []\n",
        "#     for i in range(length):\n",
        "#         lst1.extend([i]*(length-1))\n",
        "#     lst2 = []\n",
        "#     for i in range(length):\n",
        "#         for j in range(length):\n",
        "#             if i != j:\n",
        "#                 lst2.append(j)\n",
        "#     return np.array([lst1, lst2])\n",
        "\n",
        "# create_edges(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahzdsd0OoqJb",
        "outputId": "db69f7bc-6078-40e0-fcf0-0d320a7c00d5"
      },
      "id": "ahzdsd0OoqJb",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "        4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7,\n",
              "        7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9,\n",
              "        9, 9],\n",
              "       [1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 3, 4,\n",
              "        5, 6, 7, 8, 9, 0, 1, 2, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 5, 6, 7, 8,\n",
              "        9, 0, 1, 2, 3, 4, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 7, 8, 9, 0, 1, 2,\n",
              "        3, 4, 5, 6, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 9, 0, 1, 2, 3, 4, 5, 6,\n",
              "        7, 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "e6ecb529",
      "metadata": {
        "id": "e6ecb529",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2648dd18-fd53-4c3f-ae59-985eacda9091"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2]),\n",
              " Data(x=[8, 5], edge_index=[2, 7], y=[2])]"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "# import torch\n",
        "# from torch_geometric.data import Data\n",
        "# def create_graph(array):\n",
        "#     num_nodes = len(array[1])\n",
        "#     edge_indexes = torch.tensor(create_edges(num_nodes), dtype=torch.long)\n",
        "#     data = Data(x=torch.tensor(array[0], dtype=torch.double),\n",
        "#                 edge_index=edge_indexes,\n",
        "#                 y=torch.tensor(array[1]))\n",
        "#     return data\n",
        "\n",
        "# def create_graphs(array):\n",
        "#     lst = []\n",
        "#     num_nodes = len(array[1])\n",
        "#     for i in range(num_nodes):\n",
        "#         edge_indexes = torch.tensor(create_edge(num_nodes, i), dtype=torch.long)\n",
        "#         data = Data(x=torch.tensor(array[0], dtype=torch.double),\n",
        "#                     edge_index=edge_indexes,\n",
        "#                     y=torch.tensor(array[1][i]))\n",
        "#         lst.append(data)\n",
        "#     return lst\n",
        "\n",
        "# data = create_graphs(example1)\n",
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fN9Mc8_rv0WD"
      },
      "id": "fN9Mc8_rv0WD",
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import networkx as nx\n",
        "# from torch_geometric.utils import to_networkx\n",
        "# g = to_networkx(data[0], to_undirected=False)\n",
        "# nx.draw(g)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "CMYrKAonmmqS",
        "outputId": "d804ad94-3489-4430-bf03-8f6293f596ef"
      },
      "id": "CMYrKAonmmqS",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18b2874d",
      "metadata": {
        "id": "18b2874d"
      },
      "source": [
        "# Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "66774050",
      "metadata": {
        "id": "66774050"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ba598378",
      "metadata": {
        "id": "ba598378"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "95154df7",
      "metadata": {
        "id": "95154df7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "dea70d73",
      "metadata": {
        "id": "dea70d73"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "3af520ae",
      "metadata": {
        "id": "3af520ae"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e95af5f9",
      "metadata": {
        "id": "e95af5f9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "07e03ddf",
      "metadata": {
        "id": "07e03ddf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d5fb3b29",
      "metadata": {
        "id": "d5fb3b29"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "bf5fa1b4",
      "metadata": {
        "id": "bf5fa1b4"
      },
      "outputs": [],
      "source": [
        "#todo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "id": "2280031f",
      "metadata": {
        "id": "2280031f"
      },
      "outputs": [],
      "source": [
        "# pip install torch_geometric\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "id": "3a8240f1",
      "metadata": {
        "id": "3a8240f1"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Linear\n",
        "import torch.nn as nn\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(-1, 32)\n",
        "        self.conv2 = GCNConv(32, 64)\n",
        "        self.linear = Linear(64, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
        "        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
        "        x = self.linear(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.nn import Linear\n",
        "# from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "# class GCN(torch.nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.gcn = GCNConv(5, 3)\n",
        "#         self.out = Linear(3, 2)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         h = self.gcn(x, edge_index).relu()\n",
        "#         h = self.out(h)\n",
        "#         return h"
      ],
      "metadata": {
        "id": "4dlYlVJ85DCJ"
      },
      "id": "4dlYlVJ85DCJ",
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "        nn.Linear(5, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(32, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.layers(x)\n",
        "        return output"
      ],
      "metadata": {
        "id": "1U0JHosoUiai"
      },
      "id": "1U0JHosoUiai",
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class euclidean(torch.nn.Module): # forgot to define backward()\n",
        "#     def forward(self, output, target):\n",
        "#         loss = torch.norm(output-target)\n",
        "#         return loss\n",
        "\n",
        "class euclidean(torch.nn.Module): # forgot to define backward()\n",
        "    def forward(self, output, target):\n",
        "        loss = torch.norm(output-target, dim=-1)\n",
        "        return torch.sum(loss)\n",
        "\n",
        "def compute_sum_euclidean(output, target):\n",
        "    distance = 0\n",
        "    output = output.detach().numpy()\n",
        "    target = target.numpy()\n",
        "    for i in range(len(target)):\n",
        "        # print(output[i], target[i])\n",
        "        dist = np.linalg.norm(output[i] - target[i])\n",
        "        # print(dist)\n",
        "        distance += dist\n",
        "    return distance"
      ],
      "metadata": {
        "id": "7sgH0ihiyv-t"
      },
      "id": "7sgH0ihiyv-t",
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1q8EUSgEykTc"
      },
      "id": "1q8EUSgEykTc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP"
      ],
      "metadata": {
        "id": "oA2UcPiPylHG"
      },
      "id": "oA2UcPiPylHG"
    },
    {
      "cell_type": "code",
      "source": [
        "train_graphs_lst[:1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqCsX0iAzH3t",
        "outputId": "808a1071-c455-41ce-da2a-bd5d71a4b076"
      },
      "id": "xqCsX0iAzH3t",
      "execution_count": 256,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Data(x=[9, 5], edge_index=[2, 72], edge_attr=[72, 2], y=[9, 2])]"
            ]
          },
          "metadata": {},
          "execution_count": 256
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLP()\n",
        "print(model)\n",
        "\n",
        "criterion = euclidean()\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "def train(lst):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    for data in lst:\n",
        "        out = model(data.x.float())  # Perform a single forward pass.\n",
        "        loss = criterion(out.float(), data.y.float())\n",
        "        distance = compute_sum_euclidean(out, data.y)\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        return loss, distance\n",
        "\n",
        "for epoch in range(1000):\n",
        "    # data = create_graph(example1)\n",
        "    loss, dist = train(train_graphs_lst)\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}, Summed distance: {dist}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZRZmhLxynMc",
        "outputId": "a11903bd-571d-4626-e044-78d231d44e97"
      },
      "id": "FZRZmhLxynMc",
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP(\n",
            "  (layers): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=32, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch: 0, Loss: 140.65313720703125, Summed distance: 140.65313260136355\n",
            "Epoch: 1, Loss: 151.0332489013672, Summed distance: 151.03324913843596\n",
            "Epoch: 2, Loss: 125.30228424072266, Summed distance: 125.3022781862516\n",
            "Epoch: 3, Loss: 104.74185943603516, Summed distance: 104.741851150242\n",
            "Epoch: 4, Loss: 56.469722747802734, Summed distance: 56.469717333838496\n",
            "Epoch: 5, Loss: 86.6111068725586, Summed distance: 86.61111654399339\n",
            "Epoch: 6, Loss: 71.95117950439453, Summed distance: 71.95118423454352\n",
            "Epoch: 7, Loss: 41.22624206542969, Summed distance: 41.226241372994146\n",
            "Epoch: 8, Loss: 43.27727127075195, Summed distance: 43.2772680524164\n",
            "Epoch: 9, Loss: 47.84933090209961, Summed distance: 47.849326110760764\n",
            "Epoch: 10, Loss: 43.24613952636719, Summed distance: 43.24613626078228\n",
            "Epoch: 11, Loss: 48.24871063232422, Summed distance: 48.24871640508133\n",
            "Epoch: 12, Loss: 37.42133712768555, Summed distance: 37.42133972875429\n",
            "Epoch: 13, Loss: 29.900705337524414, Summed distance: 29.90070311811182\n",
            "Epoch: 14, Loss: 32.91194152832031, Summed distance: 32.91194025999917\n",
            "Epoch: 15, Loss: 26.87026023864746, Summed distance: 26.870256809768765\n",
            "Epoch: 16, Loss: 28.714988708496094, Summed distance: 28.714990960110573\n",
            "Epoch: 17, Loss: 27.886030197143555, Summed distance: 27.886031846914566\n",
            "Epoch: 18, Loss: 28.593969345092773, Summed distance: 28.593965094305442\n",
            "Epoch: 19, Loss: 28.435100555419922, Summed distance: 28.435097897488298\n",
            "Epoch: 20, Loss: 19.334064483642578, Summed distance: 19.334063715904538\n",
            "Epoch: 21, Loss: 28.904287338256836, Summed distance: 28.904290103233205\n",
            "Epoch: 22, Loss: 33.11502456665039, Summed distance: 33.115024755001194\n",
            "Epoch: 23, Loss: 24.34209442138672, Summed distance: 24.342095117209137\n",
            "Epoch: 24, Loss: 26.116785049438477, Summed distance: 26.116782041825438\n",
            "Epoch: 25, Loss: 29.032447814941406, Summed distance: 29.03244463228537\n",
            "Epoch: 26, Loss: 22.408899307250977, Summed distance: 22.40889493773879\n",
            "Epoch: 27, Loss: 25.251144409179688, Summed distance: 25.251148683691\n",
            "Epoch: 28, Loss: 26.935882568359375, Summed distance: 26.935885680060238\n",
            "Epoch: 29, Loss: 13.718924522399902, Summed distance: 13.718925780369899\n",
            "Epoch: 30, Loss: 25.962181091308594, Summed distance: 25.96217469304062\n",
            "Epoch: 31, Loss: 26.130271911621094, Summed distance: 26.130267462424573\n",
            "Epoch: 32, Loss: 25.360048294067383, Summed distance: 25.360050559394967\n",
            "Epoch: 33, Loss: 25.292423248291016, Summed distance: 25.292426506686823\n",
            "Epoch: 34, Loss: 14.739774703979492, Summed distance: 14.739778851805507\n",
            "Epoch: 35, Loss: 24.693830490112305, Summed distance: 24.6938277502829\n",
            "Epoch: 36, Loss: 28.096900939941406, Summed distance: 28.096896617403736\n",
            "Epoch: 37, Loss: 24.672658920288086, Summed distance: 24.672656155772117\n",
            "Epoch: 38, Loss: 25.885082244873047, Summed distance: 25.8850844686631\n",
            "Epoch: 39, Loss: 24.313045501708984, Summed distance: 24.313047996514218\n",
            "Epoch: 40, Loss: 16.108139038085938, Summed distance: 16.10814155334974\n",
            "Epoch: 41, Loss: 24.105913162231445, Summed distance: 24.10591123769279\n",
            "Epoch: 42, Loss: 26.452293395996094, Summed distance: 26.452288903431818\n",
            "Epoch: 43, Loss: 27.98988914489746, Summed distance: 27.9898886259122\n",
            "Epoch: 44, Loss: 27.341323852539062, Summed distance: 27.341324130842874\n",
            "Epoch: 45, Loss: 26.50899314880371, Summed distance: 26.508995049050455\n",
            "Epoch: 46, Loss: 20.251110076904297, Summed distance: 20.25111225550579\n",
            "Epoch: 47, Loss: 16.78805923461914, Summed distance: 16.7880570714203\n",
            "Epoch: 48, Loss: 18.465742111206055, Summed distance: 18.46574065241125\n",
            "Epoch: 49, Loss: 20.70629119873047, Summed distance: 20.706294096378773\n",
            "Epoch: 50, Loss: 22.307207107543945, Summed distance: 22.307208961715137\n",
            "Epoch: 51, Loss: 16.833099365234375, Summed distance: 16.83309984884796\n",
            "Epoch: 52, Loss: 16.791372299194336, Summed distance: 16.791367859634782\n",
            "Epoch: 53, Loss: 14.562152862548828, Summed distance: 14.562154255292777\n",
            "Epoch: 54, Loss: 17.808841705322266, Summed distance: 17.808842292326965\n",
            "Epoch: 55, Loss: 14.897408485412598, Summed distance: 14.89740792264506\n",
            "Epoch: 56, Loss: 16.585201263427734, Summed distance: 16.585198912082518\n",
            "Epoch: 57, Loss: 17.880247116088867, Summed distance: 17.880250509500566\n",
            "Epoch: 58, Loss: 14.98938274383545, Summed distance: 14.989382290178542\n",
            "Epoch: 59, Loss: 18.42701530456543, Summed distance: 18.42701239571085\n",
            "Epoch: 60, Loss: 13.86954116821289, Summed distance: 13.869539600360836\n",
            "Epoch: 61, Loss: 16.75815200805664, Summed distance: 16.758155839495245\n",
            "Epoch: 62, Loss: 11.427765846252441, Summed distance: 11.427767940443847\n",
            "Epoch: 63, Loss: 19.7977352142334, Summed distance: 19.79773304474569\n",
            "Epoch: 64, Loss: 21.298166275024414, Summed distance: 21.298160304279257\n",
            "Epoch: 65, Loss: 13.109898567199707, Summed distance: 13.10989579149852\n",
            "Epoch: 66, Loss: 23.657760620117188, Summed distance: 23.65776333472579\n",
            "Epoch: 67, Loss: 23.694520950317383, Summed distance: 23.694524186428858\n",
            "Epoch: 68, Loss: 12.602567672729492, Summed distance: 12.602571020381868\n",
            "Epoch: 69, Loss: 23.816679000854492, Summed distance: 23.81667505482571\n",
            "Epoch: 70, Loss: 29.81357765197754, Summed distance: 29.813574568649525\n",
            "Epoch: 71, Loss: 28.799274444580078, Summed distance: 28.799272169296195\n",
            "Epoch: 72, Loss: 18.55708885192871, Summed distance: 18.557085028860204\n",
            "Epoch: 73, Loss: 19.860260009765625, Summed distance: 19.86026282634264\n",
            "Epoch: 74, Loss: 27.151508331298828, Summed distance: 27.151514041621372\n",
            "Epoch: 75, Loss: 19.682533264160156, Summed distance: 19.682536208818068\n",
            "Epoch: 76, Loss: 14.672707557678223, Summed distance: 14.672703613534548\n",
            "Epoch: 77, Loss: 17.23886489868164, Summed distance: 17.238861919892614\n",
            "Epoch: 78, Loss: 13.9366455078125, Summed distance: 13.936642424507445\n",
            "Epoch: 79, Loss: 18.812313079833984, Summed distance: 18.812317188121494\n",
            "Epoch: 80, Loss: 24.227739334106445, Summed distance: 24.22774162325891\n",
            "Epoch: 81, Loss: 14.252870559692383, Summed distance: 14.252872889794487\n",
            "Epoch: 82, Loss: 22.325376510620117, Summed distance: 22.325374075441932\n",
            "Epoch: 83, Loss: 28.685836791992188, Summed distance: 28.685835646683195\n",
            "Epoch: 84, Loss: 26.68187713623047, Summed distance: 26.68187331046852\n",
            "Epoch: 85, Loss: 17.631357192993164, Summed distance: 17.631354040593443\n",
            "Epoch: 86, Loss: 18.388214111328125, Summed distance: 18.388217827342093\n",
            "Epoch: 87, Loss: 24.279699325561523, Summed distance: 24.27970286188873\n",
            "Epoch: 88, Loss: 20.91836929321289, Summed distance: 20.91837222874638\n",
            "Epoch: 89, Loss: 11.900679588317871, Summed distance: 11.90067575730631\n",
            "Epoch: 90, Loss: 16.860267639160156, Summed distance: 16.860263903507306\n",
            "Epoch: 91, Loss: 10.13945198059082, Summed distance: 10.139448759660532\n",
            "Epoch: 92, Loss: 23.191003799438477, Summed distance: 23.191007425190808\n",
            "Epoch: 93, Loss: 26.584285736083984, Summed distance: 26.584287602221572\n",
            "Epoch: 94, Loss: 14.715241432189941, Summed distance: 14.715244464500513\n",
            "Epoch: 95, Loss: 22.800230026245117, Summed distance: 22.800225587547594\n",
            "Epoch: 96, Loss: 29.584426879882812, Summed distance: 29.584423971783913\n",
            "Epoch: 97, Loss: 21.62632179260254, Summed distance: 21.62631625410375\n",
            "Epoch: 98, Loss: 16.112730026245117, Summed distance: 16.112725733837713\n",
            "Epoch: 99, Loss: 28.632732391357422, Summed distance: 28.632737725229415\n",
            "Epoch: 100, Loss: 32.91374588012695, Summed distance: 32.9137500140468\n",
            "Epoch: 101, Loss: 22.911680221557617, Summed distance: 22.91168576983177\n",
            "Epoch: 102, Loss: 13.957087516784668, Summed distance: 13.95709075450897\n",
            "Epoch: 103, Loss: 28.783018112182617, Summed distance: 28.783013142705002\n",
            "Epoch: 104, Loss: 35.64984130859375, Summed distance: 35.64983731401454\n",
            "Epoch: 105, Loss: 39.04240036010742, Summed distance: 39.04239857039496\n",
            "Epoch: 106, Loss: 35.21318054199219, Summed distance: 35.213178667571306\n",
            "Epoch: 107, Loss: 26.46345329284668, Summed distance: 26.463450551641987\n",
            "Epoch: 108, Loss: 12.646159172058105, Summed distance: 12.646156226915835\n",
            "Epoch: 109, Loss: 29.08224105834961, Summed distance: 29.08224500694346\n",
            "Epoch: 110, Loss: 42.35311508178711, Summed distance: 42.35311970847041\n",
            "Epoch: 111, Loss: 40.27078628540039, Summed distance: 40.27078842979046\n",
            "Epoch: 112, Loss: 29.610063552856445, Summed distance: 29.61006744323362\n",
            "Epoch: 113, Loss: 9.987883567810059, Summed distance: 9.987887819280367\n",
            "Epoch: 114, Loss: 28.97772789001465, Summed distance: 28.97772517762481\n",
            "Epoch: 115, Loss: 40.981117248535156, Summed distance: 40.98111374417858\n",
            "Epoch: 116, Loss: 45.28468322753906, Summed distance: 45.284678885393085\n",
            "Epoch: 117, Loss: 44.785884857177734, Summed distance: 44.78588137321347\n",
            "Epoch: 118, Loss: 43.340511322021484, Summed distance: 43.340507288852216\n",
            "Epoch: 119, Loss: 37.43212127685547, Summed distance: 37.4321142081557\n",
            "Epoch: 120, Loss: 24.68816375732422, Summed distance: 24.68816074072085\n",
            "Epoch: 121, Loss: 9.87881088256836, Summed distance: 9.878815136312593\n",
            "Epoch: 122, Loss: 22.180198669433594, Summed distance: 22.1802047860325\n",
            "Epoch: 123, Loss: 24.133163452148438, Summed distance: 24.133167776207014\n",
            "Epoch: 124, Loss: 16.69058609008789, Summed distance: 16.690589725489783\n",
            "Epoch: 125, Loss: 14.24041748046875, Summed distance: 14.240414040178807\n",
            "Epoch: 126, Loss: 18.866792678833008, Summed distance: 18.866787869328945\n",
            "Epoch: 127, Loss: 16.47337532043457, Summed distance: 16.47337340364762\n",
            "Epoch: 128, Loss: 10.7097749710083, Summed distance: 10.709773849505977\n",
            "Epoch: 129, Loss: 16.263980865478516, Summed distance: 16.263984256630614\n",
            "Epoch: 130, Loss: 16.64813804626465, Summed distance: 16.648140699908826\n",
            "Epoch: 131, Loss: 11.550199508666992, Summed distance: 11.550200591212127\n",
            "Epoch: 132, Loss: 15.131234169006348, Summed distance: 15.13123075651148\n",
            "Epoch: 133, Loss: 16.1321964263916, Summed distance: 16.132192826172005\n",
            "Epoch: 134, Loss: 13.51919174194336, Summed distance: 13.519188263031033\n",
            "Epoch: 135, Loss: 13.033767700195312, Summed distance: 13.033771186579424\n",
            "Epoch: 136, Loss: 14.069342613220215, Summed distance: 14.06934678179888\n",
            "Epoch: 137, Loss: 10.40127944946289, Summed distance: 10.401282080541826\n",
            "Epoch: 138, Loss: 14.465984344482422, Summed distance: 14.465981740456275\n",
            "Epoch: 139, Loss: 16.42203140258789, Summed distance: 16.422027106487707\n",
            "Epoch: 140, Loss: 14.50704574584961, Summed distance: 14.507040990652253\n",
            "Epoch: 141, Loss: 10.922739028930664, Summed distance: 10.922742186871531\n",
            "Epoch: 142, Loss: 12.94240951538086, Summed distance: 12.942412817583245\n",
            "Epoch: 143, Loss: 7.811427116394043, Summed distance: 7.811430253404892\n",
            "Epoch: 144, Loss: 16.892925262451172, Summed distance: 16.892921019872766\n",
            "Epoch: 145, Loss: 21.50452423095703, Summed distance: 21.504519785880017\n",
            "Epoch: 146, Loss: 18.387033462524414, Summed distance: 18.387031948489007\n",
            "Epoch: 147, Loss: 11.2855863571167, Summed distance: 11.285583673986352\n",
            "Epoch: 148, Loss: 18.8692569732666, Summed distance: 18.8692601177496\n",
            "Epoch: 149, Loss: 24.676654815673828, Summed distance: 24.67665706430085\n",
            "Epoch: 150, Loss: 21.082197189331055, Summed distance: 21.0822013251016\n",
            "Epoch: 151, Loss: 10.421699523925781, Summed distance: 10.421702869000343\n",
            "Epoch: 152, Loss: 18.22389793395996, Summed distance: 18.22389433859817\n",
            "Epoch: 153, Loss: 24.663272857666016, Summed distance: 24.663269797294802\n",
            "Epoch: 154, Loss: 25.57349395751953, Summed distance: 25.573491205339174\n",
            "Epoch: 155, Loss: 22.27566909790039, Summed distance: 22.27566550868314\n",
            "Epoch: 156, Loss: 11.97774600982666, Summed distance: 11.97774268365485\n",
            "Epoch: 157, Loss: 14.323838233947754, Summed distance: 14.32384217401099\n",
            "Epoch: 158, Loss: 21.843528747558594, Summed distance: 21.843531773836517\n",
            "Epoch: 159, Loss: 19.18988609313965, Summed distance: 19.189891012882896\n",
            "Epoch: 160, Loss: 10.728781700134277, Summed distance: 10.728784939651304\n",
            "Epoch: 161, Loss: 13.943859100341797, Summed distance: 13.943855007307478\n",
            "Epoch: 162, Loss: 18.976255416870117, Summed distance: 18.976250697062962\n",
            "Epoch: 163, Loss: 20.369482040405273, Summed distance: 20.3694799725662\n",
            "Epoch: 164, Loss: 13.095552444458008, Summed distance: 13.095548733861932\n",
            "Epoch: 165, Loss: 12.35610580444336, Summed distance: 12.356108625451249\n",
            "Epoch: 166, Loss: 13.639555931091309, Summed distance: 13.639559428404153\n",
            "Epoch: 167, Loss: 15.83657455444336, Summed distance: 15.836577054515125\n",
            "Epoch: 168, Loss: 11.565810203552246, Summed distance: 11.56580939348695\n",
            "Epoch: 169, Loss: 12.534273147583008, Summed distance: 12.534270418002963\n",
            "Epoch: 170, Loss: 10.370524406433105, Summed distance: 10.370521909718821\n",
            "Epoch: 171, Loss: 9.82771110534668, Summed distance: 9.82771414704216\n",
            "Epoch: 172, Loss: 10.381660461425781, Summed distance: 10.381663099106788\n",
            "Epoch: 173, Loss: 7.013371467590332, Summed distance: 7.013374106818752\n",
            "Epoch: 174, Loss: 9.970829963684082, Summed distance: 9.970826526486306\n",
            "Epoch: 175, Loss: 11.115235328674316, Summed distance: 11.11523167533284\n",
            "Epoch: 176, Loss: 5.538208484649658, Summed distance: 5.538206147118149\n",
            "Epoch: 177, Loss: 12.450926780700684, Summed distance: 12.45092998917199\n",
            "Epoch: 178, Loss: 15.977212905883789, Summed distance: 15.977215745895538\n",
            "Epoch: 179, Loss: 10.443564414978027, Summed distance: 10.443566877570703\n",
            "Epoch: 180, Loss: 8.094202041625977, Summed distance: 8.094198897388317\n",
            "Epoch: 181, Loss: 12.85365104675293, Summed distance: 12.853647556569838\n",
            "Epoch: 182, Loss: 8.744325637817383, Summed distance: 8.744322708534206\n",
            "Epoch: 183, Loss: 6.381911754608154, Summed distance: 6.381915083811826\n",
            "Epoch: 184, Loss: 7.806144714355469, Summed distance: 7.8061479962408145\n",
            "Epoch: 185, Loss: 5.169741153717041, Summed distance: 5.169740946792767\n",
            "Epoch: 186, Loss: 7.023167610168457, Summed distance: 7.023164855221959\n",
            "Epoch: 187, Loss: 6.103664875030518, Summed distance: 6.103663860722124\n",
            "Epoch: 188, Loss: 6.3308329582214355, Summed distance: 6.330835981961797\n",
            "Epoch: 189, Loss: 5.036945819854736, Summed distance: 5.036945552035686\n",
            "Epoch: 190, Loss: 7.566896915435791, Summed distance: 7.566894405678976\n",
            "Epoch: 191, Loss: 7.148382663726807, Summed distance: 7.148384476877071\n",
            "Epoch: 192, Loss: 6.089935779571533, Summed distance: 6.0899372250086135\n",
            "Epoch: 193, Loss: 6.9288330078125, Summed distance: 6.928830074771664\n",
            "Epoch: 194, Loss: 7.363635063171387, Summed distance: 7.363633530704705\n",
            "Epoch: 195, Loss: 5.913736343383789, Summed distance: 5.913739245218664\n",
            "Epoch: 196, Loss: 5.371546745300293, Summed distance: 5.371546055731152\n",
            "Epoch: 197, Loss: 6.352823734283447, Summed distance: 6.352820624238898\n",
            "Epoch: 198, Loss: 4.667904853820801, Summed distance: 4.667906537740279\n",
            "Epoch: 199, Loss: 4.78928279876709, Summed distance: 4.789286189792408\n",
            "Epoch: 200, Loss: 8.490242004394531, Summed distance: 8.490239420358217\n",
            "Epoch: 201, Loss: 7.979413032531738, Summed distance: 7.979409903245595\n",
            "Epoch: 202, Loss: 5.9000563621521, Summed distance: 5.9000605350592155\n",
            "Epoch: 203, Loss: 6.351139068603516, Summed distance: 6.35114251805611\n",
            "Epoch: 204, Loss: 6.950768947601318, Summed distance: 6.9507652107614515\n",
            "Epoch: 205, Loss: 5.665706634521484, Summed distance: 5.6657034320343636\n",
            "Epoch: 206, Loss: 5.828993320465088, Summed distance: 5.828996342737971\n",
            "Epoch: 207, Loss: 3.974508047103882, Summed distance: 3.9745115445750443\n",
            "Epoch: 208, Loss: 8.729757308959961, Summed distance: 8.729753670027801\n",
            "Epoch: 209, Loss: 9.958346366882324, Summed distance: 9.9583434195857\n",
            "Epoch: 210, Loss: 4.928593158721924, Summed distance: 4.928596612299955\n",
            "Epoch: 211, Loss: 2.0977327823638916, Summed distance: 2.097729855229162\n",
            "Epoch: 212, Loss: 9.732010841369629, Summed distance: 9.732014894620075\n",
            "Epoch: 213, Loss: 8.988750457763672, Summed distance: 8.988753565558488\n",
            "Epoch: 214, Loss: 8.87401008605957, Summed distance: 8.874006449071265\n",
            "Epoch: 215, Loss: 8.90233039855957, Summed distance: 8.902327691613854\n",
            "Epoch: 216, Loss: 5.7632269859313965, Summed distance: 5.763227092606821\n",
            "Epoch: 217, Loss: 9.589256286621094, Summed distance: 9.58925917279146\n",
            "Epoch: 218, Loss: 7.673048973083496, Summed distance: 7.673050684718769\n",
            "Epoch: 219, Loss: 7.411946773529053, Summed distance: 7.411943273061109\n",
            "Epoch: 220, Loss: 5.9480061531066895, Summed distance: 5.948002946418834\n",
            "Epoch: 221, Loss: 8.735330581665039, Summed distance: 8.735333353118264\n",
            "Epoch: 222, Loss: 9.03885269165039, Summed distance: 9.038855994616682\n",
            "Epoch: 223, Loss: 4.903491973876953, Summed distance: 4.903489044831978\n",
            "Epoch: 224, Loss: 6.250003337860107, Summed distance: 6.250001021461972\n",
            "Epoch: 225, Loss: 4.913649559020996, Summed distance: 4.913652879352896\n",
            "Epoch: 226, Loss: 5.521008491516113, Summed distance: 5.521008046619662\n",
            "Epoch: 227, Loss: 4.081679821014404, Summed distance: 4.081676799123141\n",
            "Epoch: 228, Loss: 7.223997116088867, Summed distance: 7.223999932283597\n",
            "Epoch: 229, Loss: 5.113039493560791, Summed distance: 5.113041640076269\n",
            "Epoch: 230, Loss: 9.453126907348633, Summed distance: 9.453124210492966\n",
            "Epoch: 231, Loss: 6.989051342010498, Summed distance: 6.989049188126597\n",
            "Epoch: 232, Loss: 8.161937713623047, Summed distance: 8.161940613480514\n",
            "Epoch: 233, Loss: 8.905011177062988, Summed distance: 8.90501266548902\n",
            "Epoch: 234, Loss: 8.920147895812988, Summed distance: 8.920144508952953\n",
            "Epoch: 235, Loss: 9.211719512939453, Summed distance: 9.211715510286032\n",
            "Epoch: 236, Loss: 9.862960815429688, Summed distance: 9.862963899667125\n",
            "Epoch: 237, Loss: 6.016026973724365, Summed distance: 6.016030195900424\n",
            "Epoch: 238, Loss: 8.862688064575195, Summed distance: 8.862684836379824\n",
            "Epoch: 239, Loss: 10.075176239013672, Summed distance: 10.075172433483676\n",
            "Epoch: 240, Loss: 5.107176303863525, Summed distance: 5.107179724880905\n",
            "Epoch: 241, Loss: 5.7624430656433105, Summed distance: 5.762446571986799\n",
            "Epoch: 242, Loss: 6.531395435333252, Summed distance: 6.531392772923342\n",
            "Epoch: 243, Loss: 9.285505294799805, Summed distance: 9.28550148234568\n",
            "Epoch: 244, Loss: 6.428430557250977, Summed distance: 6.428433347847998\n",
            "Epoch: 245, Loss: 4.265119552612305, Summed distance: 4.2651229014159\n",
            "Epoch: 246, Loss: 9.493152618408203, Summed distance: 9.49314934817254\n",
            "Epoch: 247, Loss: 8.970062255859375, Summed distance: 8.970058279708963\n",
            "Epoch: 248, Loss: 3.2894046306610107, Summed distance: 3.289407346059288\n",
            "Epoch: 249, Loss: 5.683258533477783, Summed distance: 5.68325907439956\n",
            "Epoch: 250, Loss: 3.7468793392181396, Summed distance: 3.746876879106659\n",
            "Epoch: 251, Loss: 6.978233337402344, Summed distance: 6.9782352557149245\n",
            "Epoch: 252, Loss: 6.606278419494629, Summed distance: 6.606279061078639\n",
            "Epoch: 253, Loss: 5.150627136230469, Summed distance: 5.150624404687683\n",
            "Epoch: 254, Loss: 6.434018611907959, Summed distance: 6.434019866939737\n",
            "Epoch: 255, Loss: 9.327698707580566, Summed distance: 9.32769887964309\n",
            "Epoch: 256, Loss: 5.307610988616943, Summed distance: 5.307608558847125\n",
            "Epoch: 257, Loss: 7.367775917053223, Summed distance: 7.367776301739387\n",
            "Epoch: 258, Loss: 8.142730712890625, Summed distance: 8.142731904563089\n",
            "Epoch: 259, Loss: 6.323080539703369, Summed distance: 6.323081467137604\n",
            "Epoch: 260, Loss: 8.000629425048828, Summed distance: 8.000627004208287\n",
            "Epoch: 261, Loss: 7.2722578048706055, Summed distance: 7.272257490552566\n",
            "Epoch: 262, Loss: 7.162608623504639, Summed distance: 7.162609418104881\n",
            "Epoch: 263, Loss: 3.355480909347534, Summed distance: 3.3554821015824223\n",
            "Epoch: 264, Loss: 5.461293697357178, Summed distance: 5.461292010955644\n",
            "Epoch: 265, Loss: 4.186524391174316, Summed distance: 4.186524918593567\n",
            "Epoch: 266, Loss: 6.455657005310059, Summed distance: 6.455658374032813\n",
            "Epoch: 267, Loss: 8.828298568725586, Summed distance: 8.828299108723767\n",
            "Epoch: 268, Loss: 5.534035682678223, Summed distance: 5.534033835799231\n",
            "Epoch: 269, Loss: 6.242679595947266, Summed distance: 6.24267735099585\n",
            "Epoch: 270, Loss: 9.768769264221191, Summed distance: 9.768771932034173\n",
            "Epoch: 271, Loss: 8.638415336608887, Summed distance: 8.638417807148254\n",
            "Epoch: 272, Loss: 7.022080898284912, Summed distance: 7.022077865693073\n",
            "Epoch: 273, Loss: 6.0784831047058105, Summed distance: 6.078480329365663\n",
            "Epoch: 274, Loss: 7.112980842590332, Summed distance: 7.112983213339746\n",
            "Epoch: 275, Loss: 6.625799655914307, Summed distance: 6.625802511229796\n",
            "Epoch: 276, Loss: 6.692958831787109, Summed distance: 6.692955245961525\n",
            "Epoch: 277, Loss: 5.945097923278809, Summed distance: 5.945095046908833\n",
            "Epoch: 278, Loss: 5.653087139129639, Summed distance: 5.65309008059363\n",
            "Epoch: 279, Loss: 5.2763872146606445, Summed distance: 5.276390881937742\n",
            "Epoch: 280, Loss: 8.31360149383545, Summed distance: 8.313597716615593\n",
            "Epoch: 281, Loss: 7.7766947746276855, Summed distance: 7.776691397822301\n",
            "Epoch: 282, Loss: 4.460753440856934, Summed distance: 4.4607525348019275\n",
            "Epoch: 283, Loss: 7.866524696350098, Summed distance: 7.866528080343307\n",
            "Epoch: 284, Loss: 7.529158115386963, Summed distance: 7.529161885533405\n",
            "Epoch: 285, Loss: 5.081486701965332, Summed distance: 5.081485163818495\n",
            "Epoch: 286, Loss: 7.462498188018799, Summed distance: 7.462495082372568\n",
            "Epoch: 287, Loss: 4.404465675354004, Summed distance: 4.404462424227918\n",
            "Epoch: 288, Loss: 9.082036972045898, Summed distance: 9.082039236349326\n",
            "Epoch: 289, Loss: 9.63167667388916, Summed distance: 9.631679574524277\n",
            "Epoch: 290, Loss: 3.6846821308135986, Summed distance: 3.6846855533919114\n",
            "Epoch: 291, Loss: 14.235611915588379, Summed distance: 14.235610468538079\n",
            "Epoch: 292, Loss: 17.837953567504883, Summed distance: 17.837951424437314\n",
            "Epoch: 293, Loss: 15.812867164611816, Summed distance: 15.812863308788453\n",
            "Epoch: 294, Loss: 9.099662780761719, Summed distance: 9.099658974411463\n",
            "Epoch: 295, Loss: 7.122330665588379, Summed distance: 7.122333548148514\n",
            "Epoch: 296, Loss: 11.48143196105957, Summed distance: 11.481435119153515\n",
            "Epoch: 297, Loss: 8.86962890625, Summed distance: 8.86963212323641\n",
            "Epoch: 298, Loss: 5.613447666168213, Summed distance: 5.613445007433193\n",
            "Epoch: 299, Loss: 7.676464080810547, Summed distance: 7.676461121725602\n",
            "Epoch: 300, Loss: 4.08703088760376, Summed distance: 4.087028076416672\n",
            "Epoch: 301, Loss: 8.5068359375, Summed distance: 8.506839540910606\n",
            "Epoch: 302, Loss: 10.621271133422852, Summed distance: 10.621273454949696\n",
            "Epoch: 303, Loss: 5.765260219573975, Summed distance: 5.7652641356684295\n",
            "Epoch: 304, Loss: 9.07288646697998, Summed distance: 9.07288327293508\n",
            "Epoch: 305, Loss: 12.430785179138184, Summed distance: 12.430781655816139\n",
            "Epoch: 306, Loss: 10.717419624328613, Summed distance: 10.717416621660766\n",
            "Epoch: 307, Loss: 3.3799729347229004, Summed distance: 3.3799706223922774\n",
            "Epoch: 308, Loss: 12.704668998718262, Summed distance: 12.704673211360447\n",
            "Epoch: 309, Loss: 16.292253494262695, Summed distance: 16.29225839727081\n",
            "Epoch: 310, Loss: 11.093062400817871, Summed distance: 11.093066509566288\n",
            "Epoch: 311, Loss: 6.255189895629883, Summed distance: 6.2551883218474025\n",
            "Epoch: 312, Loss: 11.783285140991211, Summed distance: 11.783281337324363\n",
            "Epoch: 313, Loss: 11.152192115783691, Summed distance: 11.15218742299333\n",
            "Epoch: 314, Loss: 6.479795932769775, Summed distance: 6.479795789981585\n",
            "Epoch: 315, Loss: 9.35264778137207, Summed distance: 9.35265178386204\n",
            "Epoch: 316, Loss: 8.098523139953613, Summed distance: 8.098526921395898\n",
            "Epoch: 317, Loss: 5.475046157836914, Summed distance: 5.475045743407332\n",
            "Epoch: 318, Loss: 7.264894485473633, Summed distance: 7.2648912395514085\n",
            "Epoch: 319, Loss: 4.14565896987915, Summed distance: 4.145655303808063\n",
            "Epoch: 320, Loss: 9.49315357208252, Summed distance: 9.493156891740256\n",
            "Epoch: 321, Loss: 10.39554500579834, Summed distance: 10.395547675488059\n",
            "Epoch: 322, Loss: 5.842562198638916, Summed distance: 5.842564519140322\n",
            "Epoch: 323, Loss: 8.729827880859375, Summed distance: 8.729824634330837\n",
            "Epoch: 324, Loss: 13.50472640991211, Summed distance: 13.504723570569805\n",
            "Epoch: 325, Loss: 10.166473388671875, Summed distance: 10.166470753022947\n",
            "Epoch: 326, Loss: 7.669325351715088, Summed distance: 7.669328027606066\n",
            "Epoch: 327, Loss: 9.211531639099121, Summed distance: 9.211535310224738\n",
            "Epoch: 328, Loss: 5.80236291885376, Summed distance: 5.802365487954271\n",
            "Epoch: 329, Loss: 8.435800552368164, Summed distance: 8.435795880452112\n",
            "Epoch: 330, Loss: 11.608160018920898, Summed distance: 11.608156491021571\n",
            "Epoch: 331, Loss: 6.479030132293701, Summed distance: 6.479026436244493\n",
            "Epoch: 332, Loss: 8.390002250671387, Summed distance: 8.390006442816494\n",
            "Epoch: 333, Loss: 10.322903633117676, Summed distance: 10.322906503333549\n",
            "Epoch: 334, Loss: 4.359884262084961, Summed distance: 4.359887987217438\n",
            "Epoch: 335, Loss: 11.020493507385254, Summed distance: 11.020491003231891\n",
            "Epoch: 336, Loss: 13.381962776184082, Summed distance: 13.381960195657951\n",
            "Epoch: 337, Loss: 9.518712997436523, Summed distance: 9.518708891545\n",
            "Epoch: 338, Loss: 6.262437343597412, Summed distance: 6.262438410050003\n",
            "Epoch: 339, Loss: 9.496051788330078, Summed distance: 9.496055095976152\n",
            "Epoch: 340, Loss: 6.170531272888184, Summed distance: 6.170534062947007\n",
            "Epoch: 341, Loss: 8.045546531677246, Summed distance: 8.045543625855213\n",
            "Epoch: 342, Loss: 8.70238208770752, Summed distance: 8.702378178108384\n",
            "Epoch: 343, Loss: 2.9087116718292236, Summed distance: 2.9087088165661577\n",
            "Epoch: 344, Loss: 11.68874454498291, Summed distance: 11.688747816229215\n",
            "Epoch: 345, Loss: 14.14085865020752, Summed distance: 14.140862867684895\n",
            "Epoch: 346, Loss: 7.672094345092773, Summed distance: 7.672097132696331\n",
            "Epoch: 347, Loss: 8.077792167663574, Summed distance: 8.077788771971028\n",
            "Epoch: 348, Loss: 11.240524291992188, Summed distance: 11.240521141186694\n",
            "Epoch: 349, Loss: 8.530805587768555, Summed distance: 8.530802215921105\n",
            "Epoch: 350, Loss: 4.198596477508545, Summed distance: 4.198599261182069\n",
            "Epoch: 351, Loss: 6.534170150756836, Summed distance: 6.534173652107053\n",
            "Epoch: 352, Loss: 3.595229387283325, Summed distance: 3.5952276538102237\n",
            "Epoch: 353, Loss: 4.527997970581055, Summed distance: 4.52799461898682\n",
            "Epoch: 354, Loss: 4.721858501434326, Summed distance: 4.7218619487446984\n",
            "Epoch: 355, Loss: 3.618241548538208, Summed distance: 3.618243835475985\n",
            "Epoch: 356, Loss: 5.502401828765869, Summed distance: 5.502398385435447\n",
            "Epoch: 357, Loss: 6.543005466461182, Summed distance: 6.543002592646829\n",
            "Epoch: 358, Loss: 6.420136451721191, Summed distance: 6.4201390592960434\n",
            "Epoch: 359, Loss: 4.197897911071777, Summed distance: 4.1979015400151205\n",
            "Epoch: 360, Loss: 6.16558837890625, Summed distance: 6.16558466999418\n",
            "Epoch: 361, Loss: 6.199460506439209, Summed distance: 6.199457218432637\n",
            "Epoch: 362, Loss: 3.5102436542510986, Summed distance: 3.5102459538898967\n",
            "Epoch: 363, Loss: 3.019071102142334, Summed distance: 3.0190725376546785\n",
            "Epoch: 364, Loss: 4.655744552612305, Summed distance: 4.655742258071402\n",
            "Epoch: 365, Loss: 1.9140112400054932, Summed distance: 1.9140083024041223\n",
            "Epoch: 366, Loss: 9.73719596862793, Summed distance: 9.737198883267489\n",
            "Epoch: 367, Loss: 9.977896690368652, Summed distance: 9.977900912790773\n",
            "Epoch: 368, Loss: 5.437403678894043, Summed distance: 5.437406570949382\n",
            "Epoch: 369, Loss: 9.576993942260742, Summed distance: 9.576991208757054\n",
            "Epoch: 370, Loss: 12.228286743164062, Summed distance: 12.228283634815329\n",
            "Epoch: 371, Loss: 11.736923217773438, Summed distance: 11.736919380245942\n",
            "Epoch: 372, Loss: 5.737395286560059, Summed distance: 5.73739392200268\n",
            "Epoch: 373, Loss: 11.371871948242188, Summed distance: 11.371875334703093\n",
            "Epoch: 374, Loss: 13.131446838378906, Summed distance: 13.13145033674375\n",
            "Epoch: 375, Loss: 7.312473773956299, Summed distance: 7.312476540585217\n",
            "Epoch: 376, Loss: 9.686708450317383, Summed distance: 9.686706293768136\n",
            "Epoch: 377, Loss: 10.996971130371094, Summed distance: 10.996968196265637\n",
            "Epoch: 378, Loss: 7.74404239654541, Summed distance: 7.7440392236184055\n",
            "Epoch: 379, Loss: 6.798584461212158, Summed distance: 6.798586765197277\n",
            "Epoch: 380, Loss: 8.900444984436035, Summed distance: 8.90044870610995\n",
            "Epoch: 381, Loss: 4.498623847961426, Summed distance: 4.498626903027051\n",
            "Epoch: 382, Loss: 9.775931358337402, Summed distance: 9.77592809672781\n",
            "Epoch: 383, Loss: 11.133037567138672, Summed distance: 11.133033945930787\n",
            "Epoch: 384, Loss: 11.2047119140625, Summed distance: 11.204708503421639\n",
            "Epoch: 385, Loss: 4.641778469085693, Summed distance: 4.641779330550486\n",
            "Epoch: 386, Loss: 9.606338500976562, Summed distance: 9.606340656045493\n",
            "Epoch: 387, Loss: 10.170611381530762, Summed distance: 10.170614246266421\n",
            "Epoch: 388, Loss: 6.252118110656738, Summed distance: 6.252117875198951\n",
            "Epoch: 389, Loss: 8.239104270935059, Summed distance: 8.23910059149218\n",
            "Epoch: 390, Loss: 5.98751974105835, Summed distance: 5.987516766383698\n",
            "Epoch: 391, Loss: 5.314387798309326, Summed distance: 5.3143891331339415\n",
            "Epoch: 392, Loss: 7.144954204559326, Summed distance: 7.1449568828992955\n",
            "Epoch: 393, Loss: 2.6686978340148926, Summed distance: 2.6687011938414416\n",
            "Epoch: 394, Loss: 8.877107620239258, Summed distance: 8.87710428795922\n",
            "Epoch: 395, Loss: 11.885607719421387, Summed distance: 11.885604396436252\n",
            "Epoch: 396, Loss: 9.028303146362305, Summed distance: 9.02829928003824\n",
            "Epoch: 397, Loss: 4.313149452209473, Summed distance: 4.313150548292671\n",
            "Epoch: 398, Loss: 8.331684112548828, Summed distance: 8.331688072441432\n",
            "Epoch: 399, Loss: 9.754755973815918, Summed distance: 9.754759128128201\n",
            "Epoch: 400, Loss: 7.488309860229492, Summed distance: 7.488309376742889\n",
            "Epoch: 401, Loss: 7.559494972229004, Summed distance: 7.559492035829819\n",
            "Epoch: 402, Loss: 7.456232070922852, Summed distance: 7.456229036600027\n",
            "Epoch: 403, Loss: 5.954954147338867, Summed distance: 5.954955200627971\n",
            "Epoch: 404, Loss: 8.347053527832031, Summed distance: 8.347055717655293\n",
            "Epoch: 405, Loss: 4.472100734710693, Summed distance: 4.4721042625844625\n",
            "Epoch: 406, Loss: 9.852653503417969, Summed distance: 9.852649473065457\n",
            "Epoch: 407, Loss: 11.86587142944336, Summed distance: 11.865867845314181\n",
            "Epoch: 408, Loss: 8.762738227844238, Summed distance: 8.762735523920702\n",
            "Epoch: 409, Loss: 7.1523356437683105, Summed distance: 7.152338115790407\n",
            "Epoch: 410, Loss: 7.6897053718566895, Summed distance: 7.689709067923113\n",
            "Epoch: 411, Loss: 7.68210506439209, Summed distance: 7.682105598799246\n",
            "Epoch: 412, Loss: 7.2899041175842285, Summed distance: 7.289901270528616\n",
            "Epoch: 413, Loss: 6.464056015014648, Summed distance: 6.464052401763334\n",
            "Epoch: 414, Loss: 4.911657333374023, Summed distance: 4.911659367927681\n",
            "Epoch: 415, Loss: 5.119304180145264, Summed distance: 5.1193069835807705\n",
            "Epoch: 416, Loss: 4.063592433929443, Summed distance: 4.0635890905977785\n",
            "Epoch: 417, Loss: 3.9933106899261475, Summed distance: 3.9933096583751526\n",
            "Epoch: 418, Loss: 5.465527534484863, Summed distance: 5.465531457231993\n",
            "Epoch: 419, Loss: 3.3037943840026855, Summed distance: 3.303793604134692\n",
            "Epoch: 420, Loss: 4.375103950500488, Summed distance: 4.375100577340984\n",
            "Epoch: 421, Loss: 4.8003621101379395, Summed distance: 4.800365169908784\n",
            "Epoch: 422, Loss: 3.9356837272644043, Summed distance: 3.9356853739446223\n",
            "Epoch: 423, Loss: 5.840539932250977, Summed distance: 5.840536956539242\n",
            "Epoch: 424, Loss: 3.732121229171753, Summed distance: 3.7321193527597933\n",
            "Epoch: 425, Loss: 6.125624656677246, Summed distance: 6.125628592165323\n",
            "Epoch: 426, Loss: 4.390021324157715, Summed distance: 4.390024621778309\n",
            "Epoch: 427, Loss: 6.995644569396973, Summed distance: 6.995641424374326\n",
            "Epoch: 428, Loss: 6.524844646453857, Summed distance: 6.524841641001236\n",
            "Epoch: 429, Loss: 3.517374038696289, Summed distance: 3.5173743349348676\n",
            "Epoch: 430, Loss: 6.674256324768066, Summed distance: 6.674258802597507\n",
            "Epoch: 431, Loss: 5.829431056976318, Summed distance: 5.829432694954783\n",
            "Epoch: 432, Loss: 5.607072830200195, Summed distance: 5.607069673737958\n",
            "Epoch: 433, Loss: 4.419763565063477, Summed distance: 4.41976070859645\n",
            "Epoch: 434, Loss: 6.302492618560791, Summed distance: 6.3024955511164515\n",
            "Epoch: 435, Loss: 7.1096720695495605, Summed distance: 7.109675230892503\n",
            "Epoch: 436, Loss: 2.936580181121826, Summed distance: 2.936577035819295\n",
            "Epoch: 437, Loss: 5.829503059387207, Summed distance: 5.829500983833434\n",
            "Epoch: 438, Loss: 5.864931583404541, Summed distance: 5.8649342281281385\n",
            "Epoch: 439, Loss: 4.005190372467041, Summed distance: 4.0051936979273846\n",
            "Epoch: 440, Loss: 8.18960189819336, Summed distance: 8.189598057099188\n",
            "Epoch: 441, Loss: 11.362218856811523, Summed distance: 11.362215249060228\n",
            "Epoch: 442, Loss: 7.4508795738220215, Summed distance: 7.4508779474119935\n",
            "Epoch: 443, Loss: 7.478578567504883, Summed distance: 7.478581556074719\n",
            "Epoch: 444, Loss: 7.985149383544922, Summed distance: 7.985152610727436\n",
            "Epoch: 445, Loss: 5.842527389526367, Summed distance: 5.842526666717948\n",
            "Epoch: 446, Loss: 6.721506118774414, Summed distance: 6.721503022550823\n",
            "Epoch: 447, Loss: 6.079179763793945, Summed distance: 6.079176339590146\n",
            "Epoch: 448, Loss: 9.493402481079102, Summed distance: 9.493405813362848\n",
            "Epoch: 449, Loss: 9.330317497253418, Summed distance: 9.330320403537375\n",
            "Epoch: 450, Loss: 5.410912990570068, Summed distance: 5.4109147775008415\n",
            "Epoch: 451, Loss: 9.61721420288086, Summed distance: 9.617211800335792\n",
            "Epoch: 452, Loss: 13.39614200592041, Summed distance: 13.396139204941594\n",
            "Epoch: 453, Loss: 10.38901424407959, Summed distance: 10.389011278392124\n",
            "Epoch: 454, Loss: 7.629627704620361, Summed distance: 7.629628905046362\n",
            "Epoch: 455, Loss: 9.37396240234375, Summed distance: 9.37396582781781\n",
            "Epoch: 456, Loss: 7.750818252563477, Summed distance: 7.750820764656089\n",
            "Epoch: 457, Loss: 9.745794296264648, Summed distance: 9.745791235801875\n",
            "Epoch: 458, Loss: 10.137340545654297, Summed distance: 10.137336537523213\n",
            "Epoch: 459, Loss: 7.056108474731445, Summed distance: 7.056105735117421\n",
            "Epoch: 460, Loss: 8.721419334411621, Summed distance: 8.72142292859753\n",
            "Epoch: 461, Loss: 7.71636438369751, Summed distance: 7.7163677739392105\n",
            "Epoch: 462, Loss: 5.970886707305908, Summed distance: 5.970887043714401\n",
            "Epoch: 463, Loss: 7.677366733551025, Summed distance: 7.677363438483992\n",
            "Epoch: 464, Loss: 6.623913764953613, Summed distance: 6.623910621795771\n",
            "Epoch: 465, Loss: 7.35840368270874, Summed distance: 7.358406253447507\n",
            "Epoch: 466, Loss: 5.914772033691406, Summed distance: 5.914774718320575\n",
            "Epoch: 467, Loss: 4.209188938140869, Summed distance: 4.209186052662299\n",
            "Epoch: 468, Loss: 4.446469306945801, Summed distance: 4.4464673650690285\n",
            "Epoch: 469, Loss: 4.619482517242432, Summed distance: 4.619486442477022\n",
            "Epoch: 470, Loss: 4.47224235534668, Summed distance: 4.472241678537813\n",
            "Epoch: 471, Loss: 5.060911178588867, Summed distance: 5.060907858443348\n",
            "Epoch: 472, Loss: 5.843347549438477, Summed distance: 5.843350530430662\n",
            "Epoch: 473, Loss: 3.343860387802124, Summed distance: 3.3438622298815845\n",
            "Epoch: 474, Loss: 8.178818702697754, Summed distance: 8.178814726905383\n",
            "Epoch: 475, Loss: 7.412440299987793, Summed distance: 7.412438999795966\n",
            "Epoch: 476, Loss: 5.657553195953369, Summed distance: 5.6575559633160974\n",
            "Epoch: 477, Loss: 4.554660320281982, Summed distance: 4.554663278468517\n",
            "Epoch: 478, Loss: 7.979860782623291, Summed distance: 7.979857491896444\n",
            "Epoch: 479, Loss: 11.612855911254883, Summed distance: 11.612852112659597\n",
            "Epoch: 480, Loss: 7.171385288238525, Summed distance: 7.171382167860943\n",
            "Epoch: 481, Loss: 11.929702758789062, Summed distance: 11.929706708659232\n",
            "Epoch: 482, Loss: 16.72434425354004, Summed distance: 16.72434748731608\n",
            "Epoch: 483, Loss: 12.923800468444824, Summed distance: 12.923802992089767\n",
            "Epoch: 484, Loss: 4.764164447784424, Summed distance: 4.764161397176837\n",
            "Epoch: 485, Loss: 9.210457801818848, Summed distance: 9.210453645934093\n",
            "Epoch: 486, Loss: 6.709445953369141, Summed distance: 6.70944308617146\n",
            "Epoch: 487, Loss: 7.689733982086182, Summed distance: 7.689737249937076\n",
            "Epoch: 488, Loss: 12.647911071777344, Summed distance: 12.647913547290907\n",
            "Epoch: 489, Loss: 7.930018424987793, Summed distance: 7.9300220647063355\n",
            "Epoch: 490, Loss: 10.419954299926758, Summed distance: 10.419951122151268\n",
            "Epoch: 491, Loss: 14.773345947265625, Summed distance: 14.7733423904532\n",
            "Epoch: 492, Loss: 12.497638702392578, Summed distance: 12.497635073097404\n",
            "Epoch: 493, Loss: 5.103669166564941, Summed distance: 5.103666010677859\n",
            "Epoch: 494, Loss: 12.58923625946045, Summed distance: 12.589239345939694\n",
            "Epoch: 495, Loss: 16.540719985961914, Summed distance: 16.540722581189485\n",
            "Epoch: 496, Loss: 14.1502103805542, Summed distance: 14.15021454989987\n",
            "Epoch: 497, Loss: 4.307638645172119, Summed distance: 4.307642064642362\n",
            "Epoch: 498, Loss: 10.89913272857666, Summed distance: 10.899128852896336\n",
            "Epoch: 499, Loss: 17.24078369140625, Summed distance: 17.240780795949437\n",
            "Epoch: 500, Loss: 17.005550384521484, Summed distance: 17.005546524829036\n",
            "Epoch: 501, Loss: 13.30191421508789, Summed distance: 13.301911129449868\n",
            "Epoch: 502, Loss: 5.685802936553955, Summed distance: 5.685800510813704\n",
            "Epoch: 503, Loss: 11.963451385498047, Summed distance: 11.963454701820426\n",
            "Epoch: 504, Loss: 18.01540184020996, Summed distance: 18.01540550103402\n",
            "Epoch: 505, Loss: 15.364487648010254, Summed distance: 15.364490392911586\n",
            "Epoch: 506, Loss: 9.026637077331543, Summed distance: 9.026641207980068\n",
            "Epoch: 507, Loss: 9.108959197998047, Summed distance: 9.108956163515627\n",
            "Epoch: 508, Loss: 14.08535099029541, Summed distance: 14.085347781057731\n",
            "Epoch: 509, Loss: 13.616098403930664, Summed distance: 13.616095330806655\n",
            "Epoch: 510, Loss: 9.743249893188477, Summed distance: 9.743246082084399\n",
            "Epoch: 511, Loss: 5.623656272888184, Summed distance: 5.623658997342428\n",
            "Epoch: 512, Loss: 9.687329292297363, Summed distance: 9.68733310414482\n",
            "Epoch: 513, Loss: 7.1721296310424805, Summed distance: 7.172132776796015\n",
            "Epoch: 514, Loss: 5.138455867767334, Summed distance: 5.138453144619237\n",
            "Epoch: 515, Loss: 6.773830890655518, Summed distance: 6.7738276834054725\n",
            "Epoch: 516, Loss: 3.574634075164795, Summed distance: 3.5746310076031413\n",
            "Epoch: 517, Loss: 8.344517707824707, Summed distance: 8.34452133545806\n",
            "Epoch: 518, Loss: 6.012598514556885, Summed distance: 6.012602050681759\n",
            "Epoch: 519, Loss: 6.975893020629883, Summed distance: 6.975889584683193\n",
            "Epoch: 520, Loss: 6.338203430175781, Summed distance: 6.338200979632681\n",
            "Epoch: 521, Loss: 4.031595706939697, Summed distance: 4.0315991562166005\n",
            "Epoch: 522, Loss: 3.5118496417999268, Summed distance: 3.511852670231741\n",
            "Epoch: 523, Loss: 7.930069923400879, Summed distance: 7.930065838231341\n",
            "Epoch: 524, Loss: 8.092313766479492, Summed distance: 8.092310466787614\n",
            "Epoch: 525, Loss: 3.5875892639160156, Summed distance: 3.5875920555241367\n",
            "Epoch: 526, Loss: 4.316095352172852, Summed distance: 4.3160984979637265\n",
            "Epoch: 527, Loss: 6.5691237449646, Summed distance: 6.569120332296117\n",
            "Epoch: 528, Loss: 5.674641132354736, Summed distance: 5.674637914070309\n",
            "Epoch: 529, Loss: 7.816886901855469, Summed distance: 7.816890488352811\n",
            "Epoch: 530, Loss: 6.675555229187012, Summed distance: 6.675558829703533\n",
            "Epoch: 531, Loss: 6.176417827606201, Summed distance: 6.176415061645786\n",
            "Epoch: 532, Loss: 6.200997829437256, Summed distance: 6.200994065538143\n",
            "Epoch: 533, Loss: 6.632156848907471, Summed distance: 6.632160253905891\n",
            "Epoch: 534, Loss: 4.806969165802002, Summed distance: 4.806971626898825\n",
            "Epoch: 535, Loss: 7.141831398010254, Summed distance: 7.141828164694513\n",
            "Epoch: 536, Loss: 6.592519283294678, Summed distance: 6.592517136204738\n",
            "Epoch: 537, Loss: 4.860906600952148, Summed distance: 4.860910478783487\n",
            "Epoch: 538, Loss: 4.774474620819092, Summed distance: 4.774475910208918\n",
            "Epoch: 539, Loss: 5.136382102966309, Summed distance: 5.136379068796166\n",
            "Epoch: 540, Loss: 3.702040672302246, Summed distance: 3.702040138877712\n",
            "Epoch: 541, Loss: 4.801447868347168, Summed distance: 4.8014515152735475\n",
            "Epoch: 542, Loss: 3.8545162677764893, Summed distance: 3.854516542179509\n",
            "Epoch: 543, Loss: 4.972292900085449, Summed distance: 4.972289255266316\n",
            "Epoch: 544, Loss: 3.9558265209198, Summed distance: 3.9558255502590005\n",
            "Epoch: 545, Loss: 4.70344352722168, Summed distance: 4.703447523488297\n",
            "Epoch: 546, Loss: 4.020890235900879, Summed distance: 4.020888807374931\n",
            "Epoch: 547, Loss: 4.07156229019165, Summed distance: 4.071558934441172\n",
            "Epoch: 548, Loss: 7.891173362731934, Summed distance: 7.8911770545534\n",
            "Epoch: 549, Loss: 5.09425687789917, Summed distance: 5.09426014670676\n",
            "Epoch: 550, Loss: 6.225904941558838, Summed distance: 6.225901652669314\n",
            "Epoch: 551, Loss: 5.161303520202637, Summed distance: 5.161300222544889\n",
            "Epoch: 552, Loss: 5.202948093414307, Summed distance: 5.202950987926514\n",
            "Epoch: 553, Loss: 3.5139882564544678, Summed distance: 3.5139914180104217\n",
            "Epoch: 554, Loss: 8.499197959899902, Summed distance: 8.4991945676447\n",
            "Epoch: 555, Loss: 6.875585556030273, Summed distance: 6.8755820442990645\n",
            "Epoch: 556, Loss: 2.888850450515747, Summed distance: 2.888852989356001\n",
            "Epoch: 557, Loss: 3.8280551433563232, Summed distance: 3.8280563938975085\n",
            "Epoch: 558, Loss: 4.315951347351074, Summed distance: 4.315949420514914\n",
            "Epoch: 559, Loss: 4.987472057342529, Summed distance: 4.987475230551479\n",
            "Epoch: 560, Loss: 4.8616156578063965, Summed distance: 4.861616542787696\n",
            "Epoch: 561, Loss: 4.720025539398193, Summed distance: 4.7200213494770145\n",
            "Epoch: 562, Loss: 4.971215724945068, Summed distance: 4.971216398589726\n",
            "Epoch: 563, Loss: 4.659196853637695, Summed distance: 4.659199909763288\n",
            "Epoch: 564, Loss: 3.552135944366455, Summed distance: 3.5521329069544483\n",
            "Epoch: 565, Loss: 4.191850185394287, Summed distance: 4.191849417205023\n",
            "Epoch: 566, Loss: 6.02095890045166, Summed distance: 6.020962528891195\n",
            "Epoch: 567, Loss: 4.757938385009766, Summed distance: 4.757938565828762\n",
            "Epoch: 568, Loss: 5.078878402709961, Summed distance: 5.078874624714161\n",
            "Epoch: 569, Loss: 4.900401592254639, Summed distance: 4.900403839591362\n",
            "Epoch: 570, Loss: 3.210099220275879, Summed distance: 3.2101023360005505\n",
            "Epoch: 571, Loss: 7.752634048461914, Summed distance: 7.752630909926726\n",
            "Epoch: 572, Loss: 8.433979988098145, Summed distance: 8.433977613777298\n",
            "Epoch: 573, Loss: 4.305665016174316, Summed distance: 4.305665413058006\n",
            "Epoch: 574, Loss: 5.78419303894043, Summed distance: 5.784196613913572\n",
            "Epoch: 575, Loss: 4.929653167724609, Summed distance: 4.929653848876563\n",
            "Epoch: 576, Loss: 5.936196804046631, Summed distance: 5.936193618347831\n",
            "Epoch: 577, Loss: 4.564371109008789, Summed distance: 4.564368219323049\n",
            "Epoch: 578, Loss: 6.651930809020996, Summed distance: 6.6519337113734185\n",
            "Epoch: 579, Loss: 6.66294002532959, Summed distance: 6.6629430519813395\n",
            "Epoch: 580, Loss: 3.6156210899353027, Summed distance: 3.6156181303357884\n",
            "Epoch: 581, Loss: 4.409182548522949, Summed distance: 4.409178920229102\n",
            "Epoch: 582, Loss: 4.942475318908691, Summed distance: 4.9424780807743645\n",
            "Epoch: 583, Loss: 3.428976058959961, Summed distance: 3.428979266740922\n",
            "Epoch: 584, Loss: 6.852555274963379, Summed distance: 6.852552329327581\n",
            "Epoch: 585, Loss: 7.112464904785156, Summed distance: 7.112462173695602\n",
            "Epoch: 586, Loss: 3.87703013420105, Summed distance: 3.877029193899811\n",
            "Epoch: 587, Loss: 6.816293239593506, Summed distance: 6.816296867807742\n",
            "Epoch: 588, Loss: 7.240344524383545, Summed distance: 7.24034727950622\n",
            "Epoch: 589, Loss: 4.8463053703308105, Summed distance: 4.846303291941743\n",
            "Epoch: 590, Loss: 5.122861862182617, Summed distance: 5.122859295396452\n",
            "Epoch: 591, Loss: 3.4148547649383545, Summed distance: 3.4148527530991295\n",
            "Epoch: 592, Loss: 7.630436897277832, Summed distance: 7.630441165343163\n",
            "Epoch: 593, Loss: 6.330236434936523, Summed distance: 6.330240418254308\n",
            "Epoch: 594, Loss: 3.6694490909576416, Summed distance: 3.669445449173006\n",
            "Epoch: 595, Loss: 3.8285865783691406, Summed distance: 3.828583164469633\n",
            "Epoch: 596, Loss: 6.6393866539001465, Summed distance: 6.639389210269791\n",
            "Epoch: 597, Loss: 6.041554927825928, Summed distance: 6.041558440985565\n",
            "Epoch: 598, Loss: 4.386301517486572, Summed distance: 4.38629861253436\n",
            "Epoch: 599, Loss: 4.7678022384643555, Summed distance: 4.767798817102616\n",
            "Epoch: 600, Loss: 4.66231632232666, Summed distance: 4.6623194063810045\n",
            "Epoch: 601, Loss: 3.4949841499328613, Summed distance: 3.4949872067161127\n",
            "Epoch: 602, Loss: 6.3851728439331055, Summed distance: 6.385169342882002\n",
            "Epoch: 603, Loss: 5.850417613983154, Summed distance: 5.850414991580186\n",
            "Epoch: 604, Loss: 3.134333610534668, Summed distance: 3.134335557564819\n",
            "Epoch: 605, Loss: 3.7078542709350586, Summed distance: 3.70785715208883\n",
            "Epoch: 606, Loss: 3.544450044631958, Summed distance: 3.5444480359910235\n",
            "Epoch: 607, Loss: 3.341596841812134, Summed distance: 3.3415934372051312\n",
            "Epoch: 608, Loss: 5.142466068267822, Summed distance: 5.142468870779394\n",
            "Epoch: 609, Loss: 4.768477439880371, Summed distance: 4.7684803205609265\n",
            "Epoch: 610, Loss: 5.350332260131836, Summed distance: 5.350328830922232\n",
            "Epoch: 611, Loss: 4.904811859130859, Summed distance: 4.9048082465918785\n",
            "Epoch: 612, Loss: 5.865302085876465, Summed distance: 5.8653063596679464\n",
            "Epoch: 613, Loss: 5.95701265335083, Summed distance: 5.957015742866856\n",
            "Epoch: 614, Loss: 4.26939582824707, Summed distance: 4.26939216485049\n",
            "Epoch: 615, Loss: 4.30399751663208, Summed distance: 4.303994648036272\n",
            "Epoch: 616, Loss: 5.765978813171387, Summed distance: 5.765981990924741\n",
            "Epoch: 617, Loss: 5.033238887786865, Summed distance: 5.033242651927422\n",
            "Epoch: 618, Loss: 5.884721279144287, Summed distance: 5.884717905164227\n",
            "Epoch: 619, Loss: 6.980218887329102, Summed distance: 6.980215691195781\n",
            "Epoch: 620, Loss: 1.426462173461914, Summed distance: 1.426465556256815\n",
            "Epoch: 621, Loss: 2.4777095317840576, Summed distance: 2.4777080946724523\n",
            "Epoch: 622, Loss: 2.131843328475952, Summed distance: 2.1318465908171595\n",
            "Epoch: 623, Loss: 4.7036824226379395, Summed distance: 4.703679189969574\n",
            "Epoch: 624, Loss: 3.1486480236053467, Summed distance: 3.1486452961582674\n",
            "Epoch: 625, Loss: 5.894233703613281, Summed distance: 5.894237601964087\n",
            "Epoch: 626, Loss: 5.213194847106934, Summed distance: 5.213197861615476\n",
            "Epoch: 627, Loss: 4.252474784851074, Summed distance: 4.252471891208421\n",
            "Epoch: 628, Loss: 4.582104206085205, Summed distance: 4.58210057495359\n",
            "Epoch: 629, Loss: 3.702439069747925, Summed distance: 3.7024413933634133\n",
            "Epoch: 630, Loss: 3.731062173843384, Summed distance: 3.731064784598448\n",
            "Epoch: 631, Loss: 4.40539026260376, Summed distance: 4.405386908321168\n",
            "Epoch: 632, Loss: 4.008670330047607, Summed distance: 4.008667950518326\n",
            "Epoch: 633, Loss: 4.497035503387451, Summed distance: 4.497038578793447\n",
            "Epoch: 634, Loss: 3.6057016849517822, Summed distance: 3.6057055662010935\n",
            "Epoch: 635, Loss: 4.361445426940918, Summed distance: 4.361442707838935\n",
            "Epoch: 636, Loss: 4.471579074859619, Summed distance: 4.471575769659944\n",
            "Epoch: 637, Loss: 3.408346176147461, Summed distance: 3.4083489181316065\n",
            "Epoch: 638, Loss: 2.8178744316101074, Summed distance: 2.817876879528562\n",
            "Epoch: 639, Loss: 5.057760715484619, Summed distance: 5.057756795357616\n",
            "Epoch: 640, Loss: 4.5661702156066895, Summed distance: 4.56616751496794\n",
            "Epoch: 641, Loss: 3.7159111499786377, Summed distance: 3.7159148990500284\n",
            "Epoch: 642, Loss: 5.2646307945251465, Summed distance: 5.264633920979785\n",
            "Epoch: 643, Loss: 5.295286655426025, Summed distance: 5.295282599496531\n",
            "Epoch: 644, Loss: 4.625518321990967, Summed distance: 4.625514971562749\n",
            "Epoch: 645, Loss: 4.239593029022217, Summed distance: 4.239596650421294\n",
            "Epoch: 646, Loss: 3.927926540374756, Summed distance: 3.9279296164219573\n",
            "Epoch: 647, Loss: 5.307032108306885, Summed distance: 5.307028453729286\n",
            "Epoch: 648, Loss: 6.769148826599121, Summed distance: 6.769145643228223\n",
            "Epoch: 649, Loss: 3.4529170989990234, Summed distance: 3.452919846497824\n",
            "Epoch: 650, Loss: 3.4072446823120117, Summed distance: 3.407247022961399\n",
            "Epoch: 651, Loss: 4.975112438201904, Summed distance: 4.975108921422207\n",
            "Epoch: 652, Loss: 4.75130033493042, Summed distance: 4.7512974634715714\n",
            "Epoch: 653, Loss: 6.093706130981445, Summed distance: 6.093710149683462\n",
            "Epoch: 654, Loss: 5.0769734382629395, Summed distance: 5.076976644844242\n",
            "Epoch: 655, Loss: 3.7352349758148193, Summed distance: 3.735232024798216\n",
            "Epoch: 656, Loss: 4.8576273918151855, Summed distance: 4.857623696159366\n",
            "Epoch: 657, Loss: 4.756500720977783, Summed distance: 4.756503953640627\n",
            "Epoch: 658, Loss: 3.1573355197906494, Summed distance: 3.157338856643958\n",
            "Epoch: 659, Loss: 5.407444477081299, Summed distance: 5.407441591605729\n",
            "Epoch: 660, Loss: 5.665302276611328, Summed distance: 5.665299131341302\n",
            "Epoch: 661, Loss: 3.73710298538208, Summed distance: 3.737106007793681\n",
            "Epoch: 662, Loss: 2.4645419120788574, Summed distance: 2.464544909419737\n",
            "Epoch: 663, Loss: 5.665630340576172, Summed distance: 5.66562691315068\n",
            "Epoch: 664, Loss: 6.307005405426025, Summed distance: 6.307001863288026\n",
            "Epoch: 665, Loss: 1.566908836364746, Summed distance: 1.5669128256094957\n",
            "Epoch: 666, Loss: 1.9118144512176514, Summed distance: 1.9118157972717489\n",
            "Epoch: 667, Loss: 2.926903009414673, Summed distance: 2.926899590436186\n",
            "Epoch: 668, Loss: 2.3816490173339844, Summed distance: 2.3816507609701563\n",
            "Epoch: 669, Loss: 1.4140441417694092, Summed distance: 1.41404290498428\n",
            "Epoch: 670, Loss: 3.2038798332214355, Summed distance: 3.2038818058883747\n",
            "Epoch: 671, Loss: 4.068635940551758, Summed distance: 4.068632334031471\n",
            "Epoch: 672, Loss: 3.680851697921753, Summed distance: 3.6808543811705894\n",
            "Epoch: 673, Loss: 3.4659061431884766, Summed distance: 3.465904895560695\n",
            "Epoch: 674, Loss: 2.57094144821167, Summed distance: 2.5709445842030605\n",
            "Epoch: 675, Loss: 4.7424139976501465, Summed distance: 4.742411072329617\n",
            "Epoch: 676, Loss: 2.7310292720794678, Summed distance: 2.731028295466651\n",
            "Epoch: 677, Loss: 5.242528438568115, Summed distance: 5.242531490365833\n",
            "Epoch: 678, Loss: 4.004383563995361, Summed distance: 4.004384408137324\n",
            "Epoch: 679, Loss: 5.581053256988525, Summed distance: 5.581049926886099\n",
            "Epoch: 680, Loss: 2.9937849044799805, Summed distance: 2.993782628834523\n",
            "Epoch: 681, Loss: 5.271716117858887, Summed distance: 5.271720131550132\n",
            "Epoch: 682, Loss: 4.7339324951171875, Summed distance: 4.733936138434041\n",
            "Epoch: 683, Loss: 2.968837261199951, Summed distance: 2.96883338975298\n",
            "Epoch: 684, Loss: 4.455669403076172, Summed distance: 4.45566595205024\n",
            "Epoch: 685, Loss: 4.052912712097168, Summed distance: 4.052916374939537\n",
            "Epoch: 686, Loss: 2.8673453330993652, Summed distance: 2.8673487766712094\n",
            "Epoch: 687, Loss: 5.843678951263428, Summed distance: 5.843675426254603\n",
            "Epoch: 688, Loss: 6.186927795410156, Summed distance: 6.186924399938403\n",
            "Epoch: 689, Loss: 2.2579681873321533, Summed distance: 2.257970874605856\n",
            "Epoch: 690, Loss: 2.1243176460266113, Summed distance: 2.124320948005602\n",
            "Epoch: 691, Loss: 4.2836198806762695, Summed distance: 4.283616393848263\n",
            "Epoch: 692, Loss: 2.6782567501068115, Summed distance: 2.67825311003811\n",
            "Epoch: 693, Loss: 7.079072952270508, Summed distance: 7.079076286987037\n",
            "Epoch: 694, Loss: 7.201545715332031, Summed distance: 7.2015493741596375\n",
            "Epoch: 695, Loss: 2.3021342754364014, Summed distance: 2.302137949934228\n",
            "Epoch: 696, Loss: 9.533736228942871, Summed distance: 9.533732816325669\n",
            "Epoch: 697, Loss: 12.824180603027344, Summed distance: 12.824176854792022\n",
            "Epoch: 698, Loss: 10.910593032836914, Summed distance: 10.910589591198358\n",
            "Epoch: 699, Loss: 5.385403633117676, Summed distance: 5.385401369314151\n",
            "Epoch: 700, Loss: 9.076420783996582, Summed distance: 9.076423110430671\n",
            "Epoch: 701, Loss: 11.864401817321777, Summed distance: 11.864405863326212\n",
            "Epoch: 702, Loss: 9.755058288574219, Summed distance: 9.755062006193194\n",
            "Epoch: 703, Loss: 3.949298858642578, Summed distance: 3.9492982687829254\n",
            "Epoch: 704, Loss: 9.667842864990234, Summed distance: 9.66783904711639\n",
            "Epoch: 705, Loss: 9.903670310974121, Summed distance: 9.903667096590413\n",
            "Epoch: 706, Loss: 4.673004627227783, Summed distance: 4.673002665936199\n",
            "Epoch: 707, Loss: 8.378003120422363, Summed distance: 8.378006578193677\n",
            "Epoch: 708, Loss: 9.419528007507324, Summed distance: 9.419531083814299\n",
            "Epoch: 709, Loss: 5.13886022567749, Summed distance: 5.138863311967617\n",
            "Epoch: 710, Loss: 7.696695327758789, Summed distance: 7.696691481525654\n",
            "Epoch: 711, Loss: 10.930267333984375, Summed distance: 10.930263417570618\n",
            "Epoch: 712, Loss: 7.566524982452393, Summed distance: 7.566520929912277\n",
            "Epoch: 713, Loss: 1.414322018623352, Summed distance: 1.4143203597606118\n",
            "Epoch: 714, Loss: 10.269505500793457, Summed distance: 10.269508858307063\n",
            "Epoch: 715, Loss: 12.264680862426758, Summed distance: 12.264684027523964\n",
            "Epoch: 716, Loss: 8.82359790802002, Summed distance: 8.823601060023528\n",
            "Epoch: 717, Loss: 4.210618495941162, Summed distance: 4.210616549598681\n",
            "Epoch: 718, Loss: 7.469376564025879, Summed distance: 7.469372452875966\n",
            "Epoch: 719, Loss: 6.587311744689941, Summed distance: 6.5873084943178375\n",
            "Epoch: 720, Loss: 4.457390785217285, Summed distance: 4.457390757063471\n",
            "Epoch: 721, Loss: 6.327951431274414, Summed distance: 6.327954643622305\n",
            "Epoch: 722, Loss: 4.843171119689941, Summed distance: 4.843174375130025\n",
            "Epoch: 723, Loss: 5.737690448760986, Summed distance: 5.737687605434486\n",
            "Epoch: 724, Loss: 6.8580427169799805, Summed distance: 6.858040014308496\n",
            "Epoch: 725, Loss: 3.9677207469940186, Summed distance: 3.967718055939401\n",
            "Epoch: 726, Loss: 7.21902322769165, Summed distance: 7.219026493146044\n",
            "Epoch: 727, Loss: 8.108858108520508, Summed distance: 8.108861260729563\n",
            "Epoch: 728, Loss: 4.700159072875977, Summed distance: 4.700159991907464\n",
            "Epoch: 729, Loss: 6.678019046783447, Summed distance: 6.678015439644095\n",
            "Epoch: 730, Loss: 7.450699806213379, Summed distance: 7.450696849289816\n",
            "Epoch: 731, Loss: 4.594601154327393, Summed distance: 4.594599214402682\n",
            "Epoch: 732, Loss: 7.0292768478393555, Summed distance: 7.029280168107566\n",
            "Epoch: 733, Loss: 7.337399959564209, Summed distance: 7.337403380921984\n",
            "Epoch: 734, Loss: 2.9368133544921875, Summed distance: 2.936815383697892\n",
            "Epoch: 735, Loss: 7.961167812347412, Summed distance: 7.9611646511613445\n",
            "Epoch: 736, Loss: 9.780108451843262, Summed distance: 9.780104317067781\n",
            "Epoch: 737, Loss: 6.888465404510498, Summed distance: 6.888462034636443\n",
            "Epoch: 738, Loss: 3.1884748935699463, Summed distance: 3.188478259654691\n",
            "Epoch: 739, Loss: 5.604926109313965, Summed distance: 5.604929747912295\n",
            "Epoch: 740, Loss: 1.8044304847717285, Summed distance: 1.8044324425383493\n",
            "Epoch: 741, Loss: 7.486159801483154, Summed distance: 7.48615646944104\n",
            "Epoch: 742, Loss: 8.597323417663574, Summed distance: 8.597320508635176\n",
            "Epoch: 743, Loss: 5.588981628417969, Summed distance: 5.588978771850993\n",
            "Epoch: 744, Loss: 7.111389636993408, Summed distance: 7.111392616039023\n",
            "Epoch: 745, Loss: 9.025723457336426, Summed distance: 9.025726441241499\n",
            "Epoch: 746, Loss: 4.820400714874268, Summed distance: 4.820403944855366\n",
            "Epoch: 747, Loss: 8.121323585510254, Summed distance: 8.121320078777462\n",
            "Epoch: 748, Loss: 9.77471923828125, Summed distance: 9.774715833954508\n",
            "Epoch: 749, Loss: 7.342310428619385, Summed distance: 7.342306950790787\n",
            "Epoch: 750, Loss: 5.924757480621338, Summed distance: 5.924760444582611\n",
            "Epoch: 751, Loss: 5.612858295440674, Summed distance: 5.612861738028756\n",
            "Epoch: 752, Loss: 3.0885863304138184, Summed distance: 3.0885832552313945\n",
            "Epoch: 753, Loss: 3.5708792209625244, Summed distance: 3.570882036492528\n",
            "Epoch: 754, Loss: 4.802563190460205, Summed distance: 4.802559292919632\n",
            "Epoch: 755, Loss: 4.221093654632568, Summed distance: 4.22109117566379\n",
            "Epoch: 756, Loss: 6.673786163330078, Summed distance: 6.673789442371362\n",
            "Epoch: 757, Loss: 5.8566083908081055, Summed distance: 5.856611600315363\n",
            "Epoch: 758, Loss: 5.252857685089111, Summed distance: 5.2528554143216954\n",
            "Epoch: 759, Loss: 5.019368648529053, Summed distance: 5.019365314866534\n",
            "Epoch: 760, Loss: 4.28567361831665, Summed distance: 4.285675751650108\n",
            "Epoch: 761, Loss: 3.3602352142333984, Summed distance: 3.3602384546606126\n",
            "Epoch: 762, Loss: 4.681677341461182, Summed distance: 4.68167421140896\n",
            "Epoch: 763, Loss: 3.2467246055603027, Summed distance: 3.246722352895487\n",
            "Epoch: 764, Loss: 5.203149318695068, Summed distance: 5.2031524843923656\n",
            "Epoch: 765, Loss: 3.594717025756836, Summed distance: 3.594720182989077\n",
            "Epoch: 766, Loss: 5.589953422546387, Summed distance: 5.589949637790293\n",
            "Epoch: 767, Loss: 5.964466571807861, Summed distance: 5.964463197762977\n",
            "Epoch: 768, Loss: 2.4042859077453613, Summed distance: 2.4042840472588547\n",
            "Epoch: 769, Loss: 8.09986686706543, Summed distance: 8.099870032772479\n",
            "Epoch: 770, Loss: 9.21682357788086, Summed distance: 9.216826593649506\n",
            "Epoch: 771, Loss: 5.286053657531738, Summed distance: 5.286055285209492\n",
            "Epoch: 772, Loss: 6.473604202270508, Summed distance: 6.473601285582465\n",
            "Epoch: 773, Loss: 7.084194183349609, Summed distance: 7.084190301487644\n",
            "Epoch: 774, Loss: 5.553539276123047, Summed distance: 5.553537440524778\n",
            "Epoch: 775, Loss: 5.704634189605713, Summed distance: 5.704637048318184\n",
            "Epoch: 776, Loss: 6.82553768157959, Summed distance: 6.8255410040129085\n",
            "Epoch: 777, Loss: 2.8156895637512207, Summed distance: 2.815691339418426\n",
            "Epoch: 778, Loss: 7.472955226898193, Summed distance: 7.47295206107008\n",
            "Epoch: 779, Loss: 7.299399375915527, Summed distance: 7.299395855792326\n",
            "Epoch: 780, Loss: 4.882518768310547, Summed distance: 4.882515163461106\n",
            "Epoch: 781, Loss: 7.295168876647949, Summed distance: 7.2951716123336565\n",
            "Epoch: 782, Loss: 11.517807006835938, Summed distance: 11.51781032889\n",
            "Epoch: 783, Loss: 8.137495040893555, Summed distance: 8.137498205636554\n",
            "Epoch: 784, Loss: 4.5891852378845215, Summed distance: 4.58918175272653\n",
            "Epoch: 785, Loss: 8.244156837463379, Summed distance: 8.244153551197043\n",
            "Epoch: 786, Loss: 4.591304779052734, Summed distance: 4.591301668932072\n",
            "Epoch: 787, Loss: 7.031435966491699, Summed distance: 7.03143963097447\n",
            "Epoch: 788, Loss: 8.381625175476074, Summed distance: 8.381629015188285\n",
            "Epoch: 789, Loss: 3.163579225540161, Summed distance: 3.1635829100741226\n",
            "Epoch: 790, Loss: 8.820335388183594, Summed distance: 8.82033087320297\n",
            "Epoch: 791, Loss: 10.548199653625488, Summed distance: 10.548196044890101\n",
            "Epoch: 792, Loss: 9.63874340057373, Summed distance: 9.63873999869879\n",
            "Epoch: 793, Loss: 3.7387473583221436, Summed distance: 3.738743642902003\n",
            "Epoch: 794, Loss: 11.691250801086426, Summed distance: 11.691253867551263\n",
            "Epoch: 795, Loss: 15.73978042602539, Summed distance: 15.739783857822033\n",
            "Epoch: 796, Loss: 13.681421279907227, Summed distance: 13.681424103464249\n",
            "Epoch: 797, Loss: 6.989091396331787, Summed distance: 6.989094625100676\n",
            "Epoch: 798, Loss: 8.255622863769531, Summed distance: 8.255619085846645\n",
            "Epoch: 799, Loss: 12.707191467285156, Summed distance: 12.707188208540225\n",
            "Epoch: 800, Loss: 12.883210182189941, Summed distance: 12.883206452489706\n",
            "Epoch: 801, Loss: 8.801116943359375, Summed distance: 8.801113104111606\n",
            "Epoch: 802, Loss: 3.2637336254119873, Summed distance: 3.2637360232273966\n",
            "Epoch: 803, Loss: 6.396035671234131, Summed distance: 6.396038798060265\n",
            "Epoch: 804, Loss: 4.814792156219482, Summed distance: 4.814795358181084\n",
            "Epoch: 805, Loss: 5.926327705383301, Summed distance: 5.926324787185584\n",
            "Epoch: 806, Loss: 5.944591045379639, Summed distance: 5.9445877521802535\n",
            "Epoch: 807, Loss: 3.148951292037964, Summed distance: 3.148951094811664\n",
            "Epoch: 808, Loss: 5.115515232086182, Summed distance: 5.115518622360583\n",
            "Epoch: 809, Loss: 3.135174512863159, Summed distance: 3.135177345609629\n",
            "Epoch: 810, Loss: 5.303539276123047, Summed distance: 5.303535799175514\n",
            "Epoch: 811, Loss: 6.517600059509277, Summed distance: 6.517596632220847\n",
            "Epoch: 812, Loss: 2.7143349647521973, Summed distance: 2.7143357408891777\n",
            "Epoch: 813, Loss: 4.854645729064941, Summed distance: 4.854649409429254\n",
            "Epoch: 814, Loss: 3.7075347900390625, Summed distance: 3.707535563713359\n",
            "Epoch: 815, Loss: 4.058357238769531, Summed distance: 4.0583537441404784\n",
            "Epoch: 816, Loss: 4.154694557189941, Summed distance: 4.154693392330215\n",
            "Epoch: 817, Loss: 3.811009407043457, Summed distance: 3.8110125143892537\n",
            "Epoch: 818, Loss: 2.6601126194000244, Summed distance: 2.6601142139968292\n",
            "Epoch: 819, Loss: 4.1058735847473145, Summed distance: 4.105870073730226\n",
            "Epoch: 820, Loss: 3.3657162189483643, Summed distance: 3.3657131493052317\n",
            "Epoch: 821, Loss: 5.375889301300049, Summed distance: 5.375892965899886\n",
            "Epoch: 822, Loss: 4.933691501617432, Summed distance: 4.93369436000018\n",
            "Epoch: 823, Loss: 2.846034526824951, Summed distance: 2.846031090381892\n",
            "Epoch: 824, Loss: 2.269998073577881, Summed distance: 2.2699946901094936\n",
            "Epoch: 825, Loss: 4.797307014465332, Summed distance: 4.797310995346652\n",
            "Epoch: 826, Loss: 5.275529861450195, Summed distance: 5.275533598656466\n",
            "Epoch: 827, Loss: 3.3190858364105225, Summed distance: 3.3190830189683376\n",
            "Epoch: 828, Loss: 4.018022060394287, Summed distance: 4.018018589166685\n",
            "Epoch: 829, Loss: 3.2743427753448486, Summed distance: 3.274346473264287\n",
            "Epoch: 830, Loss: 3.285489082336426, Summed distance: 3.2854926568648914\n",
            "Epoch: 831, Loss: 4.763185977935791, Summed distance: 4.763181806062953\n",
            "Epoch: 832, Loss: 5.73272705078125, Summed distance: 5.73272349490837\n",
            "Epoch: 833, Loss: 2.5564873218536377, Summed distance: 2.556489175077147\n",
            "Epoch: 834, Loss: 2.8801937103271484, Summed distance: 2.8801960816914454\n",
            "Epoch: 835, Loss: 3.407618999481201, Summed distance: 3.4076179142483376\n",
            "Epoch: 836, Loss: 2.248650550842285, Summed distance: 2.2486483578251026\n",
            "Epoch: 837, Loss: 4.032949924468994, Summed distance: 4.032953534262948\n",
            "Epoch: 838, Loss: 2.185859441757202, Summed distance: 2.1858625749540472\n",
            "Epoch: 839, Loss: 6.429662704467773, Summed distance: 6.429660032756175\n",
            "Epoch: 840, Loss: 7.289935111999512, Summed distance: 7.289932274044806\n",
            "Epoch: 841, Loss: 4.602913856506348, Summed distance: 4.602910599603183\n",
            "Epoch: 842, Loss: 5.600266456604004, Summed distance: 5.600269493884055\n",
            "Epoch: 843, Loss: 7.183076858520508, Summed distance: 7.183080899069553\n",
            "Epoch: 844, Loss: 4.098178863525391, Summed distance: 4.098180762969487\n",
            "Epoch: 845, Loss: 6.480559825897217, Summed distance: 6.48055584341311\n",
            "Epoch: 846, Loss: 8.30696964263916, Summed distance: 8.306965737791993\n",
            "Epoch: 847, Loss: 5.570595741271973, Summed distance: 5.570592074424249\n",
            "Epoch: 848, Loss: 5.637855052947998, Summed distance: 5.6378582530542785\n",
            "Epoch: 849, Loss: 7.217884540557861, Summed distance: 7.217888314031118\n",
            "Epoch: 850, Loss: 3.8391025066375732, Summed distance: 3.839106421587384\n",
            "Epoch: 851, Loss: 6.729119777679443, Summed distance: 6.729115915546156\n",
            "Epoch: 852, Loss: 9.152737617492676, Summed distance: 9.152734017803105\n",
            "Epoch: 853, Loss: 6.607203006744385, Summed distance: 6.607199796301563\n",
            "Epoch: 854, Loss: 2.6518733501434326, Summed distance: 2.65187657002178\n",
            "Epoch: 855, Loss: 3.925318479537964, Summed distance: 3.925322305971104\n",
            "Epoch: 856, Loss: 3.0373189449310303, Summed distance: 3.03731733612193\n",
            "Epoch: 857, Loss: 2.8175113201141357, Summed distance: 2.8175081889835583\n",
            "Epoch: 858, Loss: 3.8508713245391846, Summed distance: 3.850874596676061\n",
            "Epoch: 859, Loss: 2.8032822608947754, Summed distance: 2.803285576299851\n",
            "Epoch: 860, Loss: 4.6138763427734375, Summed distance: 4.6138730749301455\n",
            "Epoch: 861, Loss: 4.181415557861328, Summed distance: 4.181411530824808\n",
            "Epoch: 862, Loss: 3.4466989040374756, Summed distance: 3.4467016865200923\n",
            "Epoch: 863, Loss: 3.65697979927063, Summed distance: 3.656983085891232\n",
            "Epoch: 864, Loss: 4.941569805145264, Summed distance: 4.941567229049245\n",
            "Epoch: 865, Loss: 5.288749694824219, Summed distance: 5.288745887523994\n",
            "Epoch: 866, Loss: 2.3510067462921143, Summed distance: 2.3510099056805807\n",
            "Epoch: 867, Loss: 2.1167638301849365, Summed distance: 2.1167656739378184\n",
            "Epoch: 868, Loss: 3.8468079566955566, Summed distance: 3.846804410807392\n",
            "Epoch: 869, Loss: 2.035731315612793, Summed distance: 2.0357294455763895\n",
            "Epoch: 870, Loss: 5.020088195800781, Summed distance: 5.020092249490039\n",
            "Epoch: 871, Loss: 4.4842000007629395, Summed distance: 4.484202876595534\n",
            "Epoch: 872, Loss: 4.22471809387207, Summed distance: 4.224714740626638\n",
            "Epoch: 873, Loss: 3.9266152381896973, Summed distance: 3.926611982828806\n",
            "Epoch: 874, Loss: 4.209837913513184, Summed distance: 4.209840133215217\n",
            "Epoch: 875, Loss: 3.1305980682373047, Summed distance: 3.1306015866096306\n",
            "Epoch: 876, Loss: 5.571652412414551, Summed distance: 5.57164943940113\n",
            "Epoch: 877, Loss: 5.04116678237915, Summed distance: 5.04116374683379\n",
            "Epoch: 878, Loss: 4.091558456420898, Summed distance: 4.091561974379977\n",
            "Epoch: 879, Loss: 3.6312501430511475, Summed distance: 3.6312528844909826\n",
            "Epoch: 880, Loss: 5.075159072875977, Summed distance: 5.075155235141745\n",
            "Epoch: 881, Loss: 5.001154899597168, Summed distance: 5.001151706490749\n",
            "Epoch: 882, Loss: 3.35797119140625, Summed distance: 3.357974512689746\n",
            "Epoch: 883, Loss: 2.8169329166412354, Summed distance: 2.816936293381167\n",
            "Epoch: 884, Loss: 5.8730549812316895, Summed distance: 5.873051935959774\n",
            "Epoch: 885, Loss: 5.448861122131348, Summed distance: 5.4488581865043955\n",
            "Epoch: 886, Loss: 3.3099303245544434, Summed distance: 3.3099323576607533\n",
            "Epoch: 887, Loss: 4.048603534698486, Summed distance: 4.048606783464367\n",
            "Epoch: 888, Loss: 3.61673903465271, Summed distance: 3.6167367212321646\n",
            "Epoch: 889, Loss: 2.569291353225708, Summed distance: 2.569288285103828\n",
            "Epoch: 890, Loss: 6.2510762214660645, Summed distance: 6.251078839800545\n",
            "Epoch: 891, Loss: 5.583523273468018, Summed distance: 5.583525629847066\n",
            "Epoch: 892, Loss: 3.85320782661438, Summed distance: 3.8532064492478804\n",
            "Epoch: 893, Loss: 4.726726531982422, Summed distance: 4.726723626185188\n",
            "Epoch: 894, Loss: 5.349822998046875, Summed distance: 5.349822166410448\n",
            "Epoch: 895, Loss: 6.400564670562744, Summed distance: 6.400567903929976\n",
            "Epoch: 896, Loss: 3.0215697288513184, Summed distance: 3.021572212211934\n",
            "Epoch: 897, Loss: 6.062626838684082, Summed distance: 6.062623771551944\n",
            "Epoch: 898, Loss: 6.487361431121826, Summed distance: 6.487358017580512\n",
            "Epoch: 899, Loss: 3.076918125152588, Summed distance: 3.076915026891632\n",
            "Epoch: 900, Loss: 8.383037567138672, Summed distance: 8.38304037864732\n",
            "Epoch: 901, Loss: 9.359823226928711, Summed distance: 9.359826536250123\n",
            "Epoch: 902, Loss: 6.386265754699707, Summed distance: 6.38626851880723\n",
            "Epoch: 903, Loss: 5.536955833435059, Summed distance: 5.536952917170144\n",
            "Epoch: 904, Loss: 7.955358505249023, Summed distance: 7.955355236212391\n",
            "Epoch: 905, Loss: 5.29267692565918, Summed distance: 5.292673537914567\n",
            "Epoch: 906, Loss: 5.797558307647705, Summed distance: 5.797560903901666\n",
            "Epoch: 907, Loss: 6.928401470184326, Summed distance: 6.9284046785226945\n",
            "Epoch: 908, Loss: 3.2727134227752686, Summed distance: 3.2727168783692666\n",
            "Epoch: 909, Loss: 7.653258323669434, Summed distance: 7.65325468910094\n",
            "Epoch: 910, Loss: 10.083931922912598, Summed distance: 10.083927712928128\n",
            "Epoch: 911, Loss: 7.88903284072876, Summed distance: 7.8890292058985345\n",
            "Epoch: 912, Loss: 2.093538522720337, Summed distance: 2.0935403873511267\n",
            "Epoch: 913, Loss: 5.504528522491455, Summed distance: 5.504531748909507\n",
            "Epoch: 914, Loss: 6.344668388366699, Summed distance: 6.3446703225815355\n",
            "Epoch: 915, Loss: 4.7735185623168945, Summed distance: 4.773516452536153\n",
            "Epoch: 916, Loss: 4.596865177154541, Summed distance: 4.596862054109215\n",
            "Epoch: 917, Loss: 3.892085552215576, Summed distance: 3.8920865450108884\n",
            "Epoch: 918, Loss: 5.178858757019043, Summed distance: 5.178861449902822\n",
            "Epoch: 919, Loss: 2.277087688446045, Summed distance: 2.2770909977996325\n",
            "Epoch: 920, Loss: 8.048233985900879, Summed distance: 8.048230247659093\n",
            "Epoch: 921, Loss: 9.405603408813477, Summed distance: 9.405600008628666\n",
            "Epoch: 922, Loss: 8.08998966217041, Summed distance: 8.089986695213286\n",
            "Epoch: 923, Loss: 5.135617256164551, Summed distance: 5.135619171142658\n",
            "Epoch: 924, Loss: 5.4973578453063965, Summed distance: 5.497361286008978\n",
            "Epoch: 925, Loss: 5.599303245544434, Summed distance: 5.599305963095588\n",
            "Epoch: 926, Loss: 5.013161659240723, Summed distance: 5.013160083863668\n",
            "Epoch: 927, Loss: 5.884594917297363, Summed distance: 5.884591832403125\n",
            "Epoch: 928, Loss: 2.2435989379882812, Summed distance: 2.2435959051361873\n",
            "Epoch: 929, Loss: 7.355081081390381, Summed distance: 7.355084115172733\n",
            "Epoch: 930, Loss: 8.816290855407715, Summed distance: 8.816293532594099\n",
            "Epoch: 931, Loss: 5.431025981903076, Summed distance: 5.431028916643692\n",
            "Epoch: 932, Loss: 5.6028361320495605, Summed distance: 5.602832389088406\n",
            "Epoch: 933, Loss: 7.61962890625, Summed distance: 7.619625318181812\n",
            "Epoch: 934, Loss: 6.3282952308654785, Summed distance: 6.3282922905881644\n",
            "Epoch: 935, Loss: 4.364537239074707, Summed distance: 4.364540321974156\n",
            "Epoch: 936, Loss: 5.556839942932129, Summed distance: 5.556843584353762\n",
            "Epoch: 937, Loss: 3.3659048080444336, Summed distance: 3.365905346106299\n",
            "Epoch: 938, Loss: 4.807062149047852, Summed distance: 4.807058566696338\n",
            "Epoch: 939, Loss: 4.109814643859863, Summed distance: 4.109811577472756\n",
            "Epoch: 940, Loss: 4.713434219360352, Summed distance: 4.713437615230233\n",
            "Epoch: 941, Loss: 4.222752571105957, Summed distance: 4.222755704483405\n",
            "Epoch: 942, Loss: 2.3812968730926514, Summed distance: 2.381293413932889\n",
            "Epoch: 943, Loss: 2.548211097717285, Summed distance: 2.54820763422467\n",
            "Epoch: 944, Loss: 3.3828511238098145, Summed distance: 3.3828549023383556\n",
            "Epoch: 945, Loss: 2.875739097595215, Summed distance: 2.8757423713528727\n",
            "Epoch: 946, Loss: 4.135227203369141, Summed distance: 4.135223655743477\n",
            "Epoch: 947, Loss: 4.379184722900391, Summed distance: 4.379181032905549\n",
            "Epoch: 948, Loss: 3.170255661010742, Summed distance: 3.170258063611325\n",
            "Epoch: 949, Loss: 3.7116446495056152, Summed distance: 3.711648160101732\n",
            "Epoch: 950, Loss: 4.602200031280518, Summed distance: 4.602197191563594\n",
            "Epoch: 951, Loss: 5.107028961181641, Summed distance: 5.107026141993359\n",
            "Epoch: 952, Loss: 3.3335423469543457, Summed distance: 3.333545980415413\n",
            "Epoch: 953, Loss: 2.685365676879883, Summed distance: 2.6853682127832594\n",
            "Epoch: 954, Loss: 4.431424140930176, Summed distance: 4.431420130728571\n",
            "Epoch: 955, Loss: 4.045358657836914, Summed distance: 4.045354847447475\n",
            "Epoch: 956, Loss: 4.14609432220459, Summed distance: 4.146097311935995\n",
            "Epoch: 957, Loss: 4.889702796936035, Summed distance: 4.889705931004732\n",
            "Epoch: 958, Loss: 3.089069128036499, Summed distance: 3.089065749148492\n",
            "Epoch: 959, Loss: 3.035403251647949, Summed distance: 3.0353999168644306\n",
            "Epoch: 960, Loss: 4.996713638305664, Summed distance: 4.996716942163509\n",
            "Epoch: 961, Loss: 3.8024566173553467, Summed distance: 3.802460620731398\n",
            "Epoch: 962, Loss: 3.642108201980591, Summed distance: 3.642104733145114\n",
            "Epoch: 963, Loss: 3.938579797744751, Summed distance: 3.9385759355465972\n",
            "Epoch: 964, Loss: 3.4297754764556885, Summed distance: 3.4297786453477532\n",
            "Epoch: 965, Loss: 2.355067729949951, Summed distance: 2.3550707895700334\n",
            "Epoch: 966, Loss: 4.997812747955322, Summed distance: 4.997810006842124\n",
            "Epoch: 967, Loss: 5.055732727050781, Summed distance: 5.055730338994776\n",
            "Epoch: 968, Loss: 2.2067935466766357, Summed distance: 2.206795251205243\n",
            "Epoch: 969, Loss: 3.5683178901672363, Summed distance: 3.56832093142852\n",
            "Epoch: 970, Loss: 3.2043471336364746, Summed distance: 3.2043460951682325\n",
            "Epoch: 971, Loss: 3.1697044372558594, Summed distance: 3.169700929981843\n",
            "Epoch: 972, Loss: 2.169376850128174, Summed distance: 2.1693782591280972\n",
            "Epoch: 973, Loss: 1.819985270500183, Summed distance: 1.8199881776075029\n",
            "Epoch: 974, Loss: 3.8957436084747314, Summed distance: 3.8957403142496254\n",
            "Epoch: 975, Loss: 3.28733491897583, Summed distance: 3.2873330576450717\n",
            "Epoch: 976, Loss: 3.1955504417419434, Summed distance: 3.195553922065426\n",
            "Epoch: 977, Loss: 3.229290723800659, Summed distance: 3.229293497395086\n",
            "Epoch: 978, Loss: 4.899868488311768, Summed distance: 4.899864965745201\n",
            "Epoch: 979, Loss: 4.9965033531188965, Summed distance: 4.996500222677171\n",
            "Epoch: 980, Loss: 1.927627444267273, Summed distance: 1.927630913171991\n",
            "Epoch: 981, Loss: 1.76460599899292, Summed distance: 1.7646094254776898\n",
            "Epoch: 982, Loss: 5.097697734832764, Summed distance: 5.097694406022877\n",
            "Epoch: 983, Loss: 4.299506664276123, Summed distance: 4.299503339141954\n",
            "Epoch: 984, Loss: 3.0698094367980957, Summed distance: 3.069812257101211\n",
            "Epoch: 985, Loss: 3.0466816425323486, Summed distance: 3.0466851975201505\n",
            "Epoch: 986, Loss: 3.2685608863830566, Summed distance: 3.268557540875486\n",
            "Epoch: 987, Loss: 4.158700942993164, Summed distance: 4.158698089569604\n",
            "Epoch: 988, Loss: 2.6934661865234375, Summed distance: 2.6934701384725557\n",
            "Epoch: 989, Loss: 3.5924575328826904, Summed distance: 3.5924596114851104\n",
            "Epoch: 990, Loss: 3.8772575855255127, Summed distance: 3.8772542156636742\n",
            "Epoch: 991, Loss: 4.440795421600342, Summed distance: 4.4407922891006315\n",
            "Epoch: 992, Loss: 6.604637145996094, Summed distance: 6.604640651705562\n",
            "Epoch: 993, Loss: 5.510923385620117, Summed distance: 5.510926464066162\n",
            "Epoch: 994, Loss: 3.0004189014434814, Summed distance: 3.000415797119093\n",
            "Epoch: 995, Loss: 3.1649696826934814, Summed distance: 3.164969180920947\n",
            "Epoch: 996, Loss: 3.2418441772460938, Summed distance: 3.241847854848531\n",
            "Epoch: 997, Loss: 4.184792995452881, Summed distance: 4.184789851694406\n",
            "Epoch: 998, Loss: 3.1731374263763428, Summed distance: 3.173134006637714\n",
            "Epoch: 999, Loss: 6.129820823669434, Summed distance: 6.129824504058346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outp = model(data.x.float())\n",
        "outp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De8pJgsHyoAV",
        "outputId": "340dce15-4fd1-42f3-bbc2-52d97da902ab"
      },
      "id": "De8pJgsHyoAV",
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.2158, -5.5627],\n",
              "        [-1.1524, -9.4689],\n",
              "        [-0.3769, -7.0736],\n",
              "        [-0.3158, -6.2341],\n",
              "        [-5.9795,  2.5926],\n",
              "        [ 0.8317, -5.1172],\n",
              "        [-3.8279, 86.8146],\n",
              "        [-1.0715, -7.4846]])"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNP6AwlTzXMJ",
        "outputId": "9a3e93d3-76c5-41a2-b5b7-91e54e26ebb7"
      },
      "id": "xNP6AwlTzXMJ",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1560, -5.7285],\n",
              "        [-1.0527, -9.6791],\n",
              "        [-0.4624, -7.3590],\n",
              "        [-0.3819, -6.3529],\n",
              "        [-6.3564,  2.7568],\n",
              "        [ 0.7670, -5.2778],\n",
              "        [-5.2653, 89.5328],\n",
              "        [-0.9416, -7.6763]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UIyUCaF_IP4",
        "outputId": "796410cd-425c-4ca3-cb33-514ee7a01f9a"
      },
      "id": "5UIyUCaF_IP4",
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.5467, -5.2712,  5.0786, -1.0969, -3.6693],\n",
              "        [ 3.5751,  4.3271, -0.0810, -0.5787, -2.9597],\n",
              "        [ 2.6773, -5.1597,  5.3524,  2.4265,  1.4587],\n",
              "        [ 2.2746, -6.7958,  2.2963,  0.3042, -0.4066],\n",
              "        [ 2.3836, -2.7586,  1.7794, -0.0764, -0.3595],\n",
              "        [ 4.2860,  0.3498,  4.8286,  1.3563, -2.5576],\n",
              "        [ 3.0312, -0.5056, 15.0463, -0.9477, 14.9813],\n",
              "        [ 4.1353,  1.8134,  3.8438, -0.6990, -2.2509]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GCN"
      ],
      "metadata": {
        "id": "4kgi5Q0lzPV4"
      },
      "id": "4kgi5Q0lzPV4"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import DenseGCNConv\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = DenseGCNConv(-1, 32)\n",
        "        self.conv2 = DenseGCNConv(32, 64)\n",
        "        self.linear = Linear(64, 2)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_weight=None):\n",
        "        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv1(x, edge_index, edge_weight).relu()\n",
        "        # x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index, edge_weight).relu()\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rOuULIzqzzTw"
      },
      "id": "rOuULIzqzzTw",
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "to_dense_adj(data.edge_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mp4Z_0Ck3qP9",
        "outputId": "5ecca79b-d2b3-4016-fb42-e1037e150b42"
      },
      "id": "mp4Z_0Ck3qP9",
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z9wkKlCr34LS"
      },
      "id": "z9wkKlCr34LS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN()\n",
        "print(model)\n",
        "\n",
        "# data = create_graph(example1)\n",
        "data = create_graphs(example1)\n",
        "data = data[-2]\n",
        "adj = to_dense_adj(data.edge_index)\n",
        "model = model.float()\n",
        "# pred = model(data.x.float(), data.edge_index)\n",
        "pred = model(data.x.float(), adj)\n",
        "# pred = model(data.x.float())\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypMaBqh-tFyD",
        "outputId": "53e4b6fc-9b36-4d25-fc6f-2d6d9136edcf"
      },
      "id": "ypMaBqh-tFyD",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): DenseGCNConv(-1, 32)\n",
            "  (conv2): DenseGCNConv(32, 64)\n",
            "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.6263, -0.7112],\n",
              "         [-0.8057, -0.7798],\n",
              "         [-0.7499, -0.7679],\n",
              "         [-0.7101, -0.7259],\n",
              "         [-0.7500, -0.7565],\n",
              "         [-0.7357, -0.7320],\n",
              "         [-0.7682, -0.7627],\n",
              "         [-0.7673, -0.7506]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27SR-wnm5J-d",
        "outputId": "4df81315-a274-4e6b-dce8-bc5f0031de7a"
      },
      "id": "27SR-wnm5J-d",
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.2653, 89.5328], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = euclidean()\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "def train(data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj = to_dense_adj(data.edge_index)\n",
        "    out = model(data.x.float(), adj)  # Perform a single forward pass.\n",
        "    # out = model(data.x.float())  # Perform a single forward pass.\n",
        "    loss = criterion(out.float(), data.y.float())\n",
        "    # print(out, data.y)\n",
        "    # distance = compute_sum_euclidean(out, data.y)\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return loss\n",
        "\n",
        "for epoch in range(100):\n",
        "    # data = create_graph(example1)\n",
        "    loss = train(data)\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s86mIJ-htoZs",
        "outputId": "2ec82160-84d5-4f49-f319-c3e655ca5eb5"
      },
      "id": "s86mIJ-htoZs",
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 723.1563110351562\n",
            "Epoch: 1, Loss: 492.3763427734375\n",
            "Epoch: 2, Loss: 196.11068725585938\n",
            "Epoch: 3, Loss: 205.84585571289062\n",
            "Epoch: 4, Loss: 202.72744750976562\n",
            "Epoch: 5, Loss: 99.06678771972656\n",
            "Epoch: 6, Loss: 73.33799743652344\n",
            "Epoch: 7, Loss: 142.56549072265625\n",
            "Epoch: 8, Loss: 102.82131958007812\n",
            "Epoch: 9, Loss: 116.76934814453125\n",
            "Epoch: 10, Loss: 55.14659118652344\n",
            "Epoch: 11, Loss: 144.88626098632812\n",
            "Epoch: 12, Loss: 157.08999633789062\n",
            "Epoch: 13, Loss: 55.96236801147461\n",
            "Epoch: 14, Loss: 182.52902221679688\n",
            "Epoch: 15, Loss: 200.927001953125\n",
            "Epoch: 16, Loss: 67.44950866699219\n",
            "Epoch: 17, Loss: 154.3133087158203\n",
            "Epoch: 18, Loss: 212.97061157226562\n",
            "Epoch: 19, Loss: 189.23892211914062\n",
            "Epoch: 20, Loss: 85.13546752929688\n",
            "Epoch: 21, Loss: 140.17453002929688\n",
            "Epoch: 22, Loss: 197.0714111328125\n",
            "Epoch: 23, Loss: 106.83326721191406\n",
            "Epoch: 24, Loss: 71.95423126220703\n",
            "Epoch: 25, Loss: 127.19091796875\n",
            "Epoch: 26, Loss: 101.60877227783203\n",
            "Epoch: 27, Loss: 26.895475387573242\n",
            "Epoch: 28, Loss: 78.25076293945312\n",
            "Epoch: 29, Loss: 57.117462158203125\n",
            "Epoch: 30, Loss: 60.05777359008789\n",
            "Epoch: 31, Loss: 61.135562896728516\n",
            "Epoch: 32, Loss: 36.72821807861328\n",
            "Epoch: 33, Loss: 49.042057037353516\n",
            "Epoch: 34, Loss: 18.29170036315918\n",
            "Epoch: 35, Loss: 38.82896423339844\n",
            "Epoch: 36, Loss: 27.858102798461914\n",
            "Epoch: 37, Loss: 16.788270950317383\n",
            "Epoch: 38, Loss: 56.72721481323242\n",
            "Epoch: 39, Loss: 38.18674850463867\n",
            "Epoch: 40, Loss: 55.884056091308594\n",
            "Epoch: 41, Loss: 46.71113586425781\n",
            "Epoch: 42, Loss: 49.146400451660156\n",
            "Epoch: 43, Loss: 39.1619758605957\n",
            "Epoch: 44, Loss: 59.87895584106445\n",
            "Epoch: 45, Loss: 49.14175033569336\n",
            "Epoch: 46, Loss: 41.278289794921875\n",
            "Epoch: 47, Loss: 35.24394607543945\n",
            "Epoch: 48, Loss: 49.74597930908203\n",
            "Epoch: 49, Loss: 45.979408264160156\n",
            "Epoch: 50, Loss: 38.35105895996094\n",
            "Epoch: 51, Loss: 31.246973037719727\n",
            "Epoch: 52, Loss: 56.724021911621094\n",
            "Epoch: 53, Loss: 52.52480697631836\n",
            "Epoch: 54, Loss: 30.637048721313477\n",
            "Epoch: 55, Loss: 27.576454162597656\n",
            "Epoch: 56, Loss: 45.533119201660156\n",
            "Epoch: 57, Loss: 39.247867584228516\n",
            "Epoch: 58, Loss: 39.41600799560547\n",
            "Epoch: 59, Loss: 28.80011558532715\n",
            "Epoch: 60, Loss: 57.805076599121094\n",
            "Epoch: 61, Loss: 47.84260940551758\n",
            "Epoch: 62, Loss: 38.35333251953125\n",
            "Epoch: 63, Loss: 37.996559143066406\n",
            "Epoch: 64, Loss: 40.53278732299805\n",
            "Epoch: 65, Loss: 45.01432418823242\n",
            "Epoch: 66, Loss: 18.16012191772461\n",
            "Epoch: 67, Loss: 17.366161346435547\n",
            "Epoch: 68, Loss: 38.15284729003906\n",
            "Epoch: 69, Loss: 23.84648895263672\n",
            "Epoch: 70, Loss: 53.09516143798828\n",
            "Epoch: 71, Loss: 44.98350524902344\n",
            "Epoch: 72, Loss: 39.84769821166992\n",
            "Epoch: 73, Loss: 44.3631477355957\n",
            "Epoch: 74, Loss: 30.060327529907227\n",
            "Epoch: 75, Loss: 29.34832191467285\n",
            "Epoch: 76, Loss: 44.5431022644043\n",
            "Epoch: 77, Loss: 37.336856842041016\n",
            "Epoch: 78, Loss: 35.59497833251953\n",
            "Epoch: 79, Loss: 29.76898956298828\n",
            "Epoch: 80, Loss: 43.197044372558594\n",
            "Epoch: 81, Loss: 38.394775390625\n",
            "Epoch: 82, Loss: 37.21596145629883\n",
            "Epoch: 83, Loss: 27.40558624267578\n",
            "Epoch: 84, Loss: 49.71141052246094\n",
            "Epoch: 85, Loss: 37.297142028808594\n",
            "Epoch: 86, Loss: 41.794700622558594\n",
            "Epoch: 87, Loss: 37.462642669677734\n",
            "Epoch: 88, Loss: 40.674644470214844\n",
            "Epoch: 89, Loss: 44.30995178222656\n",
            "Epoch: 90, Loss: 29.906578063964844\n",
            "Epoch: 91, Loss: 25.915660858154297\n",
            "Epoch: 92, Loss: 50.00807189941406\n",
            "Epoch: 93, Loss: 42.37514877319336\n",
            "Epoch: 94, Loss: 31.057039260864258\n",
            "Epoch: 95, Loss: 26.54808807373047\n",
            "Epoch: 96, Loss: 44.58544921875\n",
            "Epoch: 97, Loss: 38.545372009277344\n",
            "Epoch: 98, Loss: 29.941396713256836\n",
            "Epoch: 99, Loss: 21.715179443359375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adj = to_dense_adj(data.edge_index)\n",
        "model(data.x.float(), adj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AckhE3400lHj",
        "outputId": "b4f9b35a-2f23-474b-e1fc-75c6edcc5fdc"
      },
      "id": "AckhE3400lHj",
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.7319, 94.9787],\n",
              "         [-4.7343, 95.7232],\n",
              "         [-4.7937, 96.4394],\n",
              "         [-4.6670, 93.7315],\n",
              "         [-4.7369, 95.3022],\n",
              "         [-4.7778, 96.5004],\n",
              "         [-4.4468, 89.7168],\n",
              "         [-4.8431, 97.8520]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuWJyQdL45LW",
        "outputId": "480cb693-bd92-42c1-94de-d8feb846f641"
      },
      "id": "XuWJyQdL45LW",
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-5.2653, 89.5328], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outp = model(data.x.float(), data.edge_index)\n",
        "outp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrRClzsPvSqJ",
        "outputId": "71f190eb-5594-495f-9ced-71e91a1fb0cd"
      },
      "id": "MrRClzsPvSqJ",
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046],\n",
              "        [ 0.3389, -7.7046]])"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8Jirpm6viTG",
        "outputId": "f570e07b-f030-472c-a488-7798a6237acd"
      },
      "id": "L8Jirpm6viTG",
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1560, -5.7285],\n",
              "        [-1.0527, -9.6791],\n",
              "        [-0.4624, -7.3590],\n",
              "        [-0.3819, -6.3529],\n",
              "        [-6.3564,  2.7568],\n",
              "        [ 0.7670, -5.2778],\n",
              "        [-5.2653, 89.5328],\n",
              "        [-0.9416, -7.6763]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GCN()\n",
        "print(model)\n",
        "\n",
        "# data = create_graph(example1)\n",
        "data = create_graphs(example1)\n",
        "data = data[-2]\n",
        "adj = to_dense_adj(data.edge_index)\n",
        "model = model.float()\n",
        "# pred = model(data.x.float(), data.edge_index)\n",
        "pred = model(data.x.float(), adj)\n",
        "# pred = model(data.x.float())\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4VsAPYr0oct",
        "outputId": "7fb00dad-9512-4c8f-c30b-e2c06f25312c"
      },
      "id": "Q4VsAPYr0oct",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCN(\n",
            "  (conv1): DenseGCNConv(-1, 32)\n",
            "  (conv2): DenseGCNConv(32, 64)\n",
            "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.7249,  0.1637],\n",
              "         [-0.7284,  0.1692],\n",
              "         [-0.7896,  0.0131],\n",
              "         [-0.7685,  0.0520],\n",
              "         [-0.7438,  0.1005],\n",
              "         [-0.7353,  0.1352],\n",
              "         [-0.6886,  0.0907],\n",
              "         [-0.7369,  0.1846]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = euclidean()\n",
        "# criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "def train(data):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()  # Clear gradients.\n",
        "    adj = to_dense_adj(data.edge_index)\n",
        "    out = model(data.x.float(), adj)  # Perform a single forward pass.\n",
        "    # out = model(data.x.float())  # Perform a single forward pass.\n",
        "    loss = criterion(out.float(), data.y.float())\n",
        "    # print(out, data.y)\n",
        "    # distance = compute_sum_euclidean(out, data.y)\n",
        "    loss.backward()  # Derive gradients.\n",
        "    optimizer.step()  # Update parameters based on gradients.\n",
        "    return loss\n",
        "\n",
        "for epoch in range(100):\n",
        "    # data = create_graph(example1)\n",
        "    loss = train(data)\n",
        "    print(f'Epoch: {epoch}, Loss: {loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnlsaItj7X2o",
        "outputId": "9988fe2a-b94c-474a-d7d0-24e94a1a90c7"
      },
      "id": "xnlsaItj7X2o",
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 716.2689819335938\n",
            "Epoch: 1, Loss: 507.65814208984375\n",
            "Epoch: 2, Loss: 238.046875\n",
            "Epoch: 3, Loss: 234.9832305908203\n",
            "Epoch: 4, Loss: 230.54617309570312\n",
            "Epoch: 5, Loss: 130.9409637451172\n",
            "Epoch: 6, Loss: 77.5489273071289\n",
            "Epoch: 7, Loss: 174.18505859375\n",
            "Epoch: 8, Loss: 195.310791015625\n",
            "Epoch: 9, Loss: 121.28236389160156\n",
            "Epoch: 10, Loss: 150.40902709960938\n",
            "Epoch: 11, Loss: 71.03009796142578\n",
            "Epoch: 12, Loss: 150.30050659179688\n",
            "Epoch: 13, Loss: 197.98818969726562\n",
            "Epoch: 14, Loss: 152.20135498046875\n",
            "Epoch: 15, Loss: 94.97129821777344\n",
            "Epoch: 16, Loss: 127.35231018066406\n",
            "Epoch: 17, Loss: 51.22698974609375\n",
            "Epoch: 18, Loss: 127.64900970458984\n",
            "Epoch: 19, Loss: 163.8959503173828\n",
            "Epoch: 20, Loss: 117.86187744140625\n",
            "Epoch: 21, Loss: 78.22858428955078\n",
            "Epoch: 22, Loss: 103.75994873046875\n",
            "Epoch: 23, Loss: 37.42152404785156\n",
            "Epoch: 24, Loss: 113.99258422851562\n",
            "Epoch: 25, Loss: 133.65032958984375\n",
            "Epoch: 26, Loss: 75.16162109375\n",
            "Epoch: 27, Loss: 89.73572540283203\n",
            "Epoch: 28, Loss: 104.99842834472656\n",
            "Epoch: 29, Loss: 32.825477600097656\n",
            "Epoch: 30, Loss: 103.67845153808594\n",
            "Epoch: 31, Loss: 108.44944763183594\n",
            "Epoch: 32, Loss: 30.190689086914062\n",
            "Epoch: 33, Loss: 147.90335083007812\n",
            "Epoch: 34, Loss: 193.31072998046875\n",
            "Epoch: 35, Loss: 108.78993225097656\n",
            "Epoch: 36, Loss: 64.87479400634766\n",
            "Epoch: 37, Loss: 121.67761993408203\n",
            "Epoch: 38, Loss: 104.04487609863281\n",
            "Epoch: 39, Loss: 26.008512496948242\n",
            "Epoch: 40, Loss: 106.65948486328125\n",
            "Epoch: 41, Loss: 115.12506866455078\n",
            "Epoch: 42, Loss: 37.638729095458984\n",
            "Epoch: 43, Loss: 101.38774871826172\n",
            "Epoch: 44, Loss: 135.74615478515625\n",
            "Epoch: 45, Loss: 105.28593444824219\n",
            "Epoch: 46, Loss: 28.73844337463379\n",
            "Epoch: 47, Loss: 96.94407653808594\n",
            "Epoch: 48, Loss: 99.995361328125\n",
            "Epoch: 49, Loss: 37.481632232666016\n",
            "Epoch: 50, Loss: 88.07083129882812\n",
            "Epoch: 51, Loss: 105.24415588378906\n",
            "Epoch: 52, Loss: 62.483909606933594\n",
            "Epoch: 53, Loss: 62.83984375\n",
            "Epoch: 54, Loss: 91.34281921386719\n",
            "Epoch: 55, Loss: 41.596656799316406\n",
            "Epoch: 56, Loss: 68.131103515625\n",
            "Epoch: 57, Loss: 91.97891235351562\n",
            "Epoch: 58, Loss: 53.12540054321289\n",
            "Epoch: 59, Loss: 52.81127166748047\n",
            "Epoch: 60, Loss: 76.98529052734375\n",
            "Epoch: 61, Loss: 27.57196617126465\n",
            "Epoch: 62, Loss: 80.09073638916016\n",
            "Epoch: 63, Loss: 102.74267578125\n",
            "Epoch: 64, Loss: 66.6563949584961\n",
            "Epoch: 65, Loss: 41.57231140136719\n",
            "Epoch: 66, Loss: 72.47481536865234\n",
            "Epoch: 67, Loss: 38.193382263183594\n",
            "Epoch: 68, Loss: 59.49147415161133\n",
            "Epoch: 69, Loss: 72.10823059082031\n",
            "Epoch: 70, Loss: 26.895889282226562\n",
            "Epoch: 71, Loss: 78.05574035644531\n",
            "Epoch: 72, Loss: 94.74127960205078\n",
            "Epoch: 73, Loss: 37.059814453125\n",
            "Epoch: 74, Loss: 73.36282348632812\n",
            "Epoch: 75, Loss: 103.32157135009766\n",
            "Epoch: 76, Loss: 76.19979858398438\n",
            "Epoch: 77, Loss: 23.666105270385742\n",
            "Epoch: 78, Loss: 57.20930862426758\n",
            "Epoch: 79, Loss: 40.001670837402344\n",
            "Epoch: 80, Loss: 41.04746627807617\n",
            "Epoch: 81, Loss: 42.337120056152344\n",
            "Epoch: 82, Loss: 32.59294128417969\n",
            "Epoch: 83, Loss: 31.432729721069336\n",
            "Epoch: 84, Loss: 22.386775970458984\n",
            "Epoch: 85, Loss: 14.604682922363281\n",
            "Epoch: 86, Loss: 29.26522445678711\n",
            "Epoch: 87, Loss: 18.89897918701172\n",
            "Epoch: 88, Loss: 34.40839767456055\n",
            "Epoch: 89, Loss: 28.73420524597168\n",
            "Epoch: 90, Loss: 26.029272079467773\n",
            "Epoch: 91, Loss: 28.360965728759766\n",
            "Epoch: 92, Loss: 24.532251358032227\n",
            "Epoch: 93, Loss: 33.23796463012695\n",
            "Epoch: 94, Loss: 34.151893615722656\n",
            "Epoch: 95, Loss: 20.32180404663086\n",
            "Epoch: 96, Loss: 31.397863388061523\n",
            "Epoch: 97, Loss: 28.691829681396484\n",
            "Epoch: 98, Loss: 21.644962310791016\n",
            "Epoch: 99, Loss: 21.739580154418945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outp = model(data.x.float(), adj)\n",
        "outp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6OhHNWt7Zz8",
        "outputId": "2cab9d8e-59c9-48eb-af86-16619aae1d89"
      },
      "id": "Q6OhHNWt7Zz8",
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.7307, 90.5267],\n",
              "         [-4.7248, 90.2838],\n",
              "         [-4.8014, 92.0195],\n",
              "         [-4.6700, 89.4473],\n",
              "         [-4.7312, 90.6106],\n",
              "         [-4.7837, 91.5282],\n",
              "         [-4.4394, 84.8134],\n",
              "         [-4.8333, 92.4337]]])"
            ]
          },
          "metadata": {},
          "execution_count": 213
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}